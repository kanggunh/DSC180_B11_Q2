# PSC-Passivator-Optimization

### Introduction
This project aims to optimize the discovery of small molecules that improve the stability of perovskite solar cells. By leveraging literature mining, graph-based molecular representations, and machine learning models, we seek to identify patterns that lead to successful molecules and generate deeper insights into perovskite solar cell performance. The current methods rely heavily on Edisonian experimentation, which is inefficient. Our goal is to automate and streamline this discovery process using data-driven techniques, focusing on the vast body of research already conducted in this field.

**Visit the project website:** [PSC-Passivator-Optimization](https://kanggunh.github.io/PSC-Passivator-Optimization/)

---
### ðŸ“‚ Project Structure
```
PSC-Passivator-Optimization
â”œâ”€â”€ data                                  # Collection of all data
â”‚   â”œâ”€â”€ annotations/                      
â”‚   â”œâ”€â”€ classification/                   
â”‚   â”œâ”€â”€ extraction_eval/                  
â”‚   â”œâ”€â”€ extraction_final/                 
â”‚   â”œâ”€â”€ finetuning/                       
â”‚   â”œâ”€â”€ model_results/                    
â”‚   â”œâ”€â”€ performance_results/              
â”‚   â”œâ”€â”€ prediction/                       
â”‚   â”œâ”€â”€ prompts/                          
â”‚   â”œâ”€â”€ rag_processing/                   
â”‚   â””â”€â”€ scraping_and_conversion/          
â”‚
â”œâ”€â”€ docs                                  # Website files
â”‚   â”œâ”€â”€ index.html                        
â”‚   â”œâ”€â”€ script.js                         
â”‚   â””â”€â”€ style.css                         
â”‚
â”œâ”€â”€ models                                # Model files and directories
â”‚   â”œâ”€â”€ classifier_svm.pkl                
â”‚   â”œâ”€â”€ classifier_xgb.pkl                
â”‚   â”œâ”€â”€ DeepSeek-R1-PSC-Extractor-8B      
â”‚   â”œâ”€â”€ DeepSeek-R1-PSC-Extractor-8B-8bit 
â”‚   â”œâ”€â”€ DeepSeek-R1-PSC-Extractor-8B-8bit-Schema-2
â”‚   â”œâ”€â”€ llama-3.2-3b-it-Perovskite-PaperExtractor
â”‚   â”œâ”€â”€ Llama-PSC-Extractor-3B-16bit      
â”‚   â”œâ”€â”€ LLama-PSC-Extractor-8B-8bit-Schema-2
â”‚   â””â”€â”€ scibert_psc_ner_model             
â”‚
â”œâ”€â”€ notebooks                             # Jupyter notebooks
â”‚   â”œâ”€â”€ annotation_parsing_flattened.ipynb
â”‚   â”œâ”€â”€ annotation_parsing_nested.ipynb   
â”‚   â”œâ”€â”€ annotations_EDA.ipynb             
â”‚   â”œâ”€â”€ chunked_training_creation_schema2.ipynb
â”‚   â”œâ”€â”€ chunked_training_creation.ipynb   
â”‚   â”œâ”€â”€ classifier_model_EDA.ipynb        
â”‚   â”œâ”€â”€ docling.ipynb                     
â”‚   â”œâ”€â”€ evaluation_final_finetuned.ipynb  
â”‚   â”œâ”€â”€ evaluation_final_newschema.ipynb  
â”‚   â”œâ”€â”€ evaluation_original.ipynb         
â”‚   â”œâ”€â”€ extraction_vis.ipynb              
â”‚   â”œâ”€â”€ prediction_vis.ipynb              
â”‚   â””â”€â”€ prediction2_vis.ipynb             
â”‚
â”œâ”€â”€ reports                               # Reports and figures
â”‚   â””â”€â”€ figures/                          
â”‚
â”œâ”€â”€ src                                   # Source code files
â”‚   â”œâ”€â”€ __init__.py                       
â”‚   â”œâ”€â”€ chunked_training_creation.py      
â”‚   â”œâ”€â”€ classification.py                 
â”‚   â”œâ”€â”€ crossref_dataset_generation.py    
â”‚   â”œâ”€â”€ db_processing.py                  
â”‚   â”œâ”€â”€ extraction_evaluation.py          
â”‚   â”œâ”€â”€ extraction.py                     
â”‚   â”œâ”€â”€ finetuning.py                     
â”‚   â”œâ”€â”€ format_extraction.py              
â”‚   â”œâ”€â”€ pdf_conversion.py                 
â”‚   â”œâ”€â”€ prediction.py                     
â”‚   â”œâ”€â”€ rag_filtering.py                  
â”‚   â””â”€â”€ scraping.py                       
â”‚
â”œâ”€â”€ requirements.txt                      # Python dependencies
â””â”€â”€ run.py                                # Main execution script
```

### Structure Organization Explained
- **Data Organization (`data/`):** All data is stored according to the specific section of the pipeline it relates to. This segmentation helps streamline data processing and analysis.
- **Notebooks Directory (`notebooks/`):** The `notebooks` folder contains Jupyter notebooks primarily for:
  - Exploratory Data Analysis (EDA)
  - Visualization creation
  - Code experimentation and prototyping that is not directly part of the automated pipeline
- **Source Code (`src/`):** All code directly related to the pipeline is modularized and includes clear docstrings for documentation. Each script handles a specific part of the pipeline, from data scraping to prediction.


---

## Running the project
**1. Install the dependencies:** run the following command from the root directory of the project: 
```
pip install -r requirements.txt
```
**2. Set Up Environment Variables:** Create a .env file in the root directory and add the following:
```
HF_TOKEN=your_huggingface_token
DEEPSEEK_API_KEY=your_deepseek_api_key
OPENAI_API_KEY=your_openai_api_key
```
**3. Set Up GROBID for PDF Conversion**
- install [docker](https://docs.docker.com/engine/install/),
- Run the following command to download and run GROBID's image:
```
docker run --rm --init --ulimit core=0 -p 8070:8070 lfoppiano/grobid:0.8.1
```
- This will initialize GROBID on http://localhost:8070.

**4. Run the Full Pipeline**  
- To execute all steps of the pipeline sequentially (scraping, classification, extraction, and prediction), use:
```
python run.py all
```

## Run Specific Stages Individually
**1. Scraping and Conversion**  
Generates a dataset of DOIs, scrapes research papers, converts PDFs to XML, and formats them into CSV.
```
python run.py scraping_and_conversion
```
**2. Classification**  
Trains the classification models and classifies research papers as relevant or not.
```
python run.py classification
```
**3. Fine-Tuning the Extraction Model**  
Fine-tune the extraction model using a chunked dataset.
```
python run.py finetuning
```

**4. Extraction for Evaluation**  
Run the extraction pipeline on annotated papers, apply RAG filtering, and format the output.
```
python run.py extraction_evaluation
```

**5. Full Extraction Pipeline**  
Execute the extraction process on the relevant papers identified by the classification model.
```
python run.py extraction
```
**6. Prediction**  
Use the extracted and formatted data to generate stability predictions.
```
python run.py prediction
```
---
## Version Information
- v1: Does not use a RAG pipeline and relies on the Llama 3B model. This version is simpler but less efficient in handling complex extraction tasks.
- v2: Uses a RAG for filtering, a fine-tuned Llama 8B model, and Schema 2 for extraction. This version had improvements over v1 but lacked accuracy in extraction.
- v3 (main branch): The latest version, which uses a RAG for filtering, DeepSeek 8B 8-bit model, and Schema 2 for extraction. This version offers improved accuracy over the previous two versions.



