# PSC-Passivator-Optimization

### Introduction
This project aims to optimize the discovery of small molecules that improve the stability of perovskite solar cells. By leveraging literature mining, graph-based molecular representations, and machine learning models, we seek to identify patterns that lead to successful molecules and generate deeper insights into perovskite solar cell performance. The current methods rely heavily on Edisonian experimentation, which is inefficient. Our goal is to automate and streamline this discovery process using data-driven techniques, focusing on the vast body of research already conducted in this field.

**Visit the project website:** [PSC-Passivator-Optimization](https://kanggunh.github.io/PSC-Passivator-Optimization/)

---
### ðŸ“‚ Project Structure
```
PSC-Passivator-Optimization
â”œâ”€â”€ data                                  # Collection of all data
â”‚   â”œâ”€â”€ annotations/                      
â”‚   â”œâ”€â”€ classification/                   
â”‚   â”œâ”€â”€ extraction_eval/                  
â”‚   â”œâ”€â”€ extraction_final/                 
â”‚   â”œâ”€â”€ finetuning/                       
â”‚   â”œâ”€â”€ model_results/                    
â”‚   â”œâ”€â”€ performance_results/              
â”‚   â”œâ”€â”€ prediction/                       
â”‚   â”œâ”€â”€ prompts/                          
â”‚   â”œâ”€â”€ rag_processing/                   
â”‚   â””â”€â”€ scraping_and_conversion/          
â”‚
â”œâ”€â”€ docs                                  # Documentation files
â”‚   â”œâ”€â”€ index.html                        
â”‚   â”œâ”€â”€ script.js                         
â”‚   â””â”€â”€ style.css                         
â”‚
â”œâ”€â”€ models                                # Model files and directories
â”‚   â”œâ”€â”€ classifier_svm.pkl                
â”‚   â”œâ”€â”€ classifier_xgb.pkl                
â”‚   â”œâ”€â”€ DeepSeek-R1-PSC-Extractor-8B      
â”‚   â”œâ”€â”€ DeepSeek-R1-PSC-Extractor-8B-8bit 
â”‚   â”œâ”€â”€ DeepSeek-R1-PSC-Extractor-8B-8bit-Schema-2
â”‚   â”œâ”€â”€ llama-3.2-3b-it-Perovskite-PaperExtractor
â”‚   â”œâ”€â”€ Llama-PSC-Extractor-3B-16bit      
â”‚   â”œâ”€â”€ LLama-PSC-Extractor-8B-8bit-Schema-2
â”‚   â””â”€â”€ scibert_psc_ner_model             
â”‚
â”œâ”€â”€ notebooks                             # Jupyter notebooks
â”‚   â”œâ”€â”€ annotation_parsing_flattened.ipynb
â”‚   â”œâ”€â”€ annotation_parsing_nested.ipynb   
â”‚   â”œâ”€â”€ annotations_EDA.ipynb             
â”‚   â”œâ”€â”€ chunked_training_creation_schema2.ipynb
â”‚   â”œâ”€â”€ chunked_training_creation.ipynb   
â”‚   â”œâ”€â”€ classifier_model_EDA.ipynb        
â”‚   â”œâ”€â”€ docling.ipynb                     
â”‚   â”œâ”€â”€ evaluation_final_finetuned.ipynb  
â”‚   â”œâ”€â”€ evaluation_final_newschema.ipynb  
â”‚   â”œâ”€â”€ evaluation_original.ipynb         
â”‚   â”œâ”€â”€ extraction_vis.ipynb              
â”‚   â”œâ”€â”€ prediction_vis.ipynb              
â”‚   â””â”€â”€ prediction2_vis.ipynb             
â”‚
â”œâ”€â”€ reports                               # Reports and figures
â”‚   â””â”€â”€ figures/                          
â”‚
â”œâ”€â”€ src                                   # Source code files
â”‚   â”œâ”€â”€ __init__.py                       
â”‚   â”œâ”€â”€ chunked_training_creation.py      
â”‚   â”œâ”€â”€ classification.py                 
â”‚   â”œâ”€â”€ crossref_dataset_generation.py    
â”‚   â”œâ”€â”€ db_processing.py                  
â”‚   â”œâ”€â”€ extraction_evaluation.py          
â”‚   â”œâ”€â”€ extraction.py                     
â”‚   â”œâ”€â”€ finetuning.py                     
â”‚   â”œâ”€â”€ format_extraction.py              
â”‚   â”œâ”€â”€ pdf_conversion.py                 
â”‚   â”œâ”€â”€ prediction.py                     
â”‚   â”œâ”€â”€ rag_filtering.py                  
â”‚   â””â”€â”€ scraping.py                       
â”‚
â”œâ”€â”€ requirements.txt                      # Python dependencies
â””â”€â”€ run.py                                # Main execution script
```
---

## Running the project
**1. Install the dependencies:** run the following command from the root directory of the project: 
```
pip install -r requirements.txt
```
**2. Set Up Environment Variables:** Create a .env file in the root directory and add the following:
```
HF_TOKEN=your_huggingface_token
DEEPSEEK_API_KEY=your_deepseek_api_key
OPENAI_API_KEY=your_openai_api_key
```
**3. Set Up GROBID for PDF Conversion**
- install [docker](https://docs.docker.com/engine/install/),
- Run the following command to download and run GROBID's image:
```
docker run --rm --init --ulimit core=0 -p 8070:8070 lfoppiano/grobid:0.8.1
```
- This will initialize GROBID on http://localhost:8070.
## Using run.py

**1. Run the Full Pipeline:** <code>python run.py all</code>

**2. Run Specific Stages:**
- <code>python run.py scraping_and_conversion</code>
- <code>python run.py classification</code>
- <code>python run.py finetuning</code>
- <code>python run.py extraction_evaluation</code>
- <code>python run.py extraction</code>
- <code>python run.py prediction</code>






