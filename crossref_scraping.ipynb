{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "from io import BytesIO\n",
    "import pymupdf\n",
    "import os\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import tempfile\n",
    "import time\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import grobid_tei_xml\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 1000 records (Offset: 0)\n",
      "Fetched 1000 records (Offset: 1000)\n"
     ]
    }
   ],
   "source": [
    "query = \"perovskite solar halide passivation\"\n",
    "base_url = f\"https://api.crossref.org/works\"\n",
    "\n",
    "rows_per_request = 1000\n",
    "offset = 0\n",
    "all_dois = []\n",
    "\n",
    "while offset < 2000:\n",
    "    url = f\"{base_url}?filter=from-pub-date:2024&query={query}&rows={rows_per_request}&offset={offset}\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed request at offset {offset}: {response.status_code}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    items = data['message']['items']\n",
    "\n",
    "    if not items:  # Stop when there are no more results\n",
    "        break\n",
    "\n",
    "    # Extract DOIs\n",
    "    for item in items:\n",
    "        if 'DOI' in item:\n",
    "            all_dois.append(item)\n",
    "\n",
    "    print(f\"Fetched {len(items)} records (Offset: {offset})\")\n",
    "    offset += rows_per_request  # Move to the next batch\n",
    "    time.sleep(1)  # Be polite and avoid rate limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_dois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.282"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean([True if 'abstract' in doi else False for doi in all_dois])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = '/Users/nicco/source/repos/DSC180_B11_Q2/data/pdfs' # os.getcwd() + '/data/pdfs'\n",
    "chrome_options = uc.ChromeOptions()\n",
    "chrome_options.add_experimental_option(\"prefs\", {\n",
    "    \"download.default_directory\": download_dir,  # Set download location\n",
    "    \"download.prompt_for_download\": False,       # Disable download prompts\n",
    "    \"plugins.always_open_pdf_externally\": True   # Download PDFs instead of opening them\n",
    "})\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = uc.Chrome(service=service, options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_url(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    # Reconstruct URL without query parameters and fragment\n",
    "    return urlunparse((parsed_url.scheme, parsed_url.netloc, parsed_url.path, '', '', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_pdf_if_button(driver):\n",
    "    try:\n",
    "        driver.find_element(By.XPATH, \"//embed[contains(@type, 'application/pdf')]\")\n",
    "        return True\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        for iframe in driver.find_elements(By.TAG_NAME, \"iframe\"):\n",
    "            try:\n",
    "                frame_type = iframe.get_attribute(\"type\")\n",
    "                if frame_type == \"application/pdf\":\n",
    "                    driver.get(iframe.get_attribute(\"src\"))\n",
    "                    return True\n",
    "            except:\n",
    "                print(f\"Failed to get link {iframe}\")\n",
    "                return False\n",
    "    except:\n",
    "        print(\"No open button found for current PDF\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf_urls(url, paper_index):\n",
    "\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    pdfs_unique = set()\n",
    "    pdf_links = []\n",
    "    pdf_pattern = re.compile(r'(?<!e)\\.pdf$|/pdf/|/articlepdf/|/article-pdf/', re.IGNORECASE)\n",
    "    for link in driver.find_elements(By.TAG_NAME, \"a\"):\n",
    "        try:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"scholar.google\" not in href and pdf_pattern.search(href): #selenium cannot download epdfs\n",
    "                base_url = get_base_url(href)\n",
    "                print(base_url)\n",
    "                if base_url not in pdfs_unique:\n",
    "                    pdfs_unique.add(base_url)\n",
    "                    pdf_links.append(href)\n",
    "        except:\n",
    "            print(f\"Failed to get link {link}\")\n",
    "    i = 0\n",
    "    if len(pdf_links) == 0:\n",
    "        print(f\"No PDF links found for paper {url}\")\n",
    "        return\n",
    "    for pdf_link in pdf_links:\n",
    "        # Ensure each link is a full URL\n",
    "        pdf_url = pdf_link if pdf_link.startswith('http') else get_base_url(url) + pdf_link\n",
    "        if \"pdf\" not in pdf_url: #skips non-pdfs after base url is used\n",
    "            continue\n",
    "        try:\n",
    "            curr_url = driver.current_url\n",
    "            driver.get(pdf_url)\n",
    "            downloaded = True\n",
    "            if curr_url != driver.current_url: # redirected to another page\n",
    "                downloaded = open_pdf_if_button(driver)\n",
    "            print(downloaded)\n",
    "            # if downloaded:\n",
    "            #     time.sleep(1)\n",
    "            #     os.chdir(download_dir)\n",
    "            #     files = filter(os.path.isfile , os.listdir(download_dir)) \n",
    "            #     files = [os.path.join(download_dir, f) for f in files]\n",
    "            #     files.sort(key=lambda x: os.path.getmtime(x))\n",
    "            #     newest_file = files[-1]\n",
    "            #     os.rename(newest_file, f\"{paper_index}_{i}.pdf\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"Skipping invalid PDF at {pdf_url}\")\n",
    "            continue\n",
    "    return pdf_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_data = all_dois[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pubs.rsc.org/en/content/articlepdf/2024/qm/d4qm00560k\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for row in initial_data[5:]:\n",
    "    url = row[\"URL\"]\n",
    "    url = \"https://pubs.rsc.org/en/content/articlelanding/2024/qm/d4qm00560k\"\n",
    "    download_pdf_urls(url, row[\"DOI\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert PDFs to XML using GROBID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grobid_url = \"http://localhost:8070/api/processFulltextDocument\"\n",
    "xml_names = os.listdir(\"../../data/xmls\")\n",
    "\n",
    "for pdf_file in os.listdir(\"../../data/pdfs\"):\n",
    "    \n",
    "    #only looks at pdf files\n",
    "    if pdf_file.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(\"../../data/pdfs\", pdf_file)\n",
    "        #doe not convert already converted files\n",
    "        if pdf_path.replace('.pdf', '.xml') in xml_names:\n",
    "            continue\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            #GROBID must be running on port 8070 for this to work\n",
    "            response = requests.post(\n",
    "                grobid_url,\n",
    "                files={'input': file},\n",
    "                headers={'Accept': 'application/xml'}\n",
    "            )\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                xml_file_path = os.path.join('../../data/xmls', pdf_file.replace('.pdf', '.xml'))\n",
    "                with open(xml_file_path, 'w', encoding='utf-8') as xml_file:\n",
    "                    xml_file.write(response.text)\n",
    "            else:\n",
    "                print(f\"Failed to convert {pdf_file}. Status code: {response.status_code}\")\n",
    "                print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
