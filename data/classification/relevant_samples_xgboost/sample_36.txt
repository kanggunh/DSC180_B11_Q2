Paper #: 13964_1


Figures S1 to S16 Tables  S1 to S30  Note S1 Machine learning methods All the code is written in Python while runs on the 2.5 GHz 13 th Gen Intel® Core(TM) i5-13400F CPU, and we utilize the scikit-learn package to build logistic regression (LR) and linear discriminant analysis (LDA), support vector machines (SVC), k-nearest neighbor clustering (KNN), neural networks (MLP), random forest (RF), decision tree (Dtree), extra trees (Etrees),and AdaBoost. XGBoost, LightGBM, and CatBoost is implemented by calling the xgboost, lightgbm, and catboost packages, respectively. We implement T-Distributed Stochastic Neighbor Embedding by calling the manifold of scikit-learn package. When we conduct feature selection, we directly retain the most important features through the model, and this is achieved using scikit-learn's "SelectFromModel". Subsequently, we establish the relationship with the target values again based on the features selected by the model, assessing the necessity of feature selection. When discussing "The disturbance of the model", we achieve it by changing the random state of the model. When discussing "The disturbance of the training data", we achieve it by adjusting the random state when partitioning the dataset. During the training of base models, all base models go through crossvalidation on the same training dataset. We divide the training data into 10 subsets and use one of them as a validation set to assess the model's performance, while the rest serve as the training set for model learning. This approach helps the models to learn the generalization features of the data and mitigates the risk of a chance optimal result. We then calculate the accuracy on each validation set and balance the trade-off between bias and variance, considering the accuracy as a measure of model performance. The model with the highest average accuracy is selected as a candidate base model. Considering that redundant features can seriously affect model accuracy, as we did not perform feature selection. As a result, XGBoost, with its regularization capability, is selected as one of the candidate base models. When choosing the final model, we expanded the primary boosting tree algorithms to balance performance and cost, adding LightGBM and CatBoost models to the mix. LightGBM stands out for its efficiency, constructing decision trees faster with lower memory consumption, while CatBoost excels in reducing overfitting risks through automatic hyperparameter optimization and Bayesian regularization (bayes_opt package). These three tree models can adjust sample weights and apply regularization techniques, improving the model's learning capabilities to some extent. We performed ten-fold cross-validation on the augmented training set and used Bayesian optimization to select model hyperparameters, enhancing the model's generalization performance. We initially defined a relatively broad hyperparameter space, iteratively refined it, and arrived at the following optimization space: For XGBoost:  ['max_depth': (3, 160) , 'learning_rate': (0.01, 0.5), 'n_estimators': (50, 500), 'gamma': (0, 5), 'min_child_weight': (0, 20), 'subsample': (0.5, 1), 'colsample_bytree': (0.3, 1), 'reg_lambda': (0, 20), 'reg_alpha': (0, 30). 'reg_alpha' and 'reg_ lambda' represents the strength of L1 and L2 regularization, respectively. For LightGBM: 'num_leaves': (1, 100), 'bagging_fraction': (0.1, 1), 'bagging_freq': (1, 20), 'feature_fraction': (0.1, 1), 'learning_rate': (0.01, 0.1), 'n_estimators': (100, 1000), 'max_depth': (3, 100), 'min_child_samples':  (5, 50) . For CatBoost: 'depth': (3, 10), 'l2_leaf_reg': (0.1, 10), 'learning_rate': (0.01, 0.1), 'bagging_temperature': (0, 1)]. It's important to be mindful of the data type of hyperparameters during the optimization process. After hyperparameter optimization, the final model is selected based on its performance on the test dataset. It's worth noting that when data augmentation is needed, we used the SMOTE algorithm (imblearn package) to enhance the training dataset while leaving the test dataset unchanged. 
 Cross-feature To enhance the interpretability of the model for each material feature or process feature, we begin by one-hot encoding each category of descriptors. For example, if there are initially 'n' descriptors, , After encoding, , . i, j, and k represent the feature lengths of the first, second, and n-th categories of descriptors, respectively. To simulate the effects of time information based on physical priors, we create cross features. Here, we only incorporate binary feature relationships. In general, features of the same category do not cross: Where m and l represent natural numbers less than the feature lengths of classes A and B, respectively. Considering the correlation of multiple features may further improve model performance, but it will also make feature engineering more complex. 
 XGBoost XGBoost is an ensemble machine learning algorithm based on boosting trees. The structure of each tree is determined by gain-based selection. New trees are generated by minimizing the objective function, gradually learning the residuals to make the predictions approach the actual values. The objective function was often described in Equation (  2 ): (1) (2) 
 Where represents the true value of the sample indexed as i, represents the predictions made by the first t-1 trees for the sample indexed as i, and represents the prediction made by the newly added tree for the sample indexed as . is a function related to complexity, and it is the innovation that sets XGBoost apart from previous gradient boosting algorithms. It controls the structure of and limits the scores of nodes. Where T is the total number of leaf nodes in the tree, represents the prediction value for the j-th leaf node. serves as a form of pre-pruning during tree construction, mitigating overfitting. represents the regularization strength, applying penalties to reduce model complexity. XGBoost is widely acclaimed for its robust handling capabilities concerning missing values, noisy data, and complex data, in part due to its support for data sampling, which further enhances its effectiveness in leveraging the dataset. 
 Synthetic minority over-sampling technique The synthetic minority over-sampling technique (SMOTE) algorithm traverses each minority sample point and, for each point as the base point, calculates the Euclidean distance to other points, such as point , assuming the dataset has d feature dimensions. Based on the magnitude of the Euclidean distance to each point, select n nearest neighbors. From these neighbors, randomly choose one neighbor , and interpolate to generate a new sample relative to the base sample according to the following relationship: In this way, by synthesizing minority class samples to balance the dataset, the model's excessive preference for the majority class is alleviated, and the model's performance on the minority class is improved. 
 Shapley additive explanations analysis shapley additive explanations analysis (SHAP) is a globally interpretable method rooted in game theory that can explain complex machine learning models. Where represents the explanation of the model for the prediction of sample , represents the model's baseline (typically the mean of the target variable for all samples). d denotes the feature length of sample , and represents the (5) contribution of the j-th feature to the prediction of the i-th sample, which is the SHAP value. SHAP values reflect the magnitude and direction of a feature's contribution. 
 Bayesian optimization The Bayesian optimization algorithm is a sequential model that efficiently leverages historical information from previously sampled points during the learning process of the objective function. It strategically selects the next sampling point based on certain criteria to reduce unnecessary sampling and achieve stronger convergence. In contrast to grid search and random search, the Bayesian optimization algorithm effectively strikes a balance between exploration and exploitation, avoiding situations where the model gets stuck in local optima. It can find the optimal solution within a small number of iterations. "Exploration" refers to the model's evaluation of areas that haven't been sampled yet, which can be determined using the covariance function of the Gaussian process. "Exploitation" refers to the sampling of regions in the posterior distribution that show the most potential. This is done when the mean function of the Gaussian process is relatively certain, and the covariance function is relatively small. 
 Evaluation metrics In the model evaluation, we simultaneously consider accuracy, precision, and specificity. For a classification problem, accuracy can be defined as follows: Where TP (True Positive) is defined as the number of positive samples predicted as positive, FP (False Positive) is defined as the number of negative samples predicted as positive, FN (False Negative) is defined as the number of positive samples predicted as negative, and TN (True Negative) is the number of negative samples predicted as negative. Precision is a function of TP and FP: Specificity is a function of TN and FP: Recall is a function of TP and FN: The F1 score is the harmonic mean of recall and precision: (6) (7) (8) (9) In multi classification problems, there are two types of F1 scores, namely micro F1 score and macro F1 score. Considering the imbalanced distribution of data in the dataset, we use micro F1 score. For a regression problem, Root Mean Squared Error (RMSE) and R-squared (R 2 ) are applied to evaluate the performances of ML algorithms. Where N is the number of samples, is the true value of i-th sample, is the predicted value of the i-th sample. is the mean value of . 
 t-Distributed Stochastic Neighbor Embedding T-Distributed Stochastic Neighbor Embedding (t-SNE) is a non-linear dimensionality reduction technique used to map high-dimensional data to a low dimensional space, while preserving the local and global structures between data points and visualizing the complex relationships of high-dimensional data. In our work, high-dimensional perovskite solar cells data , where n is the number of samples, and d is the length of the feature. Supposing there is another feature space (l<d), and for the joint probabilities in the highdimensional data : For low-dimensional space , Student t-distribution with one degree of freedom is employed to define the joint probabilities : (10) (11) (12) (11) (12) Finally, the cost function C is the sum of Kullback-Leibler divergences (KL) over all datapoints and gradient descent method is used to minimize the C. C is given by (13) (  14 ) 
 Note S2. The features and objectives analyst The features we used are perovskite materials (Perovskite), electron transport layer materials (ETL), the second layer of ETL (ETL_2), hole transport layer materials (HTL), perovskite precursor solution (Precursor_solution), deposition method (Deposition_method) and procedure (Deposition_procedure), anti-solvent treatment (Anti_solvent_treatment), and HTL additives (HTL additives).          The distribution of data has improved after introducing cross-features, as seen in Figures  S3a, c  compared to Figures  S3b, d . Locally, there is a more pronounced clustering phenomenon, indicating that more generalizable features among the data have been successfully identified, causing similar data points to converge. Globally, class 2 is more separated from class 1, increasing dissimilarity between different classes (an increase in different-class distances), thereby enhancing the model's discriminative ability. Comparing Figures  S3a, b  to Figures  S3c, d , after decoupling the time features, the clustering phenomenon in class 1 has weakened, becoming more dispersed. However, the distribution of class 2 remains largely unchanged, likely due to the substantial impact of time-features on class 1. Comparing Figures  S3a, b, c  to Figure  S3d , class 0 exhibits a more concentrated distribution in Figure  S3d , yet it remains challenging to distinguish it from class 1. Under different conditions, the distribution is more scattered and chaotic. This suggests that low PCE devices are the primary factor influencing model accuracy.  To further evaluate the impact of training data on specific machine learning algorithms, we employed the Dtrees for detection and found that this inconsistency in information (training and testing sets) adversely affects the model. Additionally, we discovered that the testing set tends to exhibit unstable optimal values, which is detrimental to model generalization. Therefore, it is essential to complement the analysis with K-fold crossvalidation to gain further insights into the algorithm's performance.          Table  S27 . The hyperparameters of the XGBoost model during the model development phase. 
 Direct relationship between features and objectives 
 Cross-tabulation for features and objectives 
 Model The hyperparameter values after Bayesian optimization. XGBoost colsample_bytree: 1.0, gamma: 0, learning_rate: 0.5, max_depth: 46, min_child_weight: 0.0, n_estimators:357, reg_alpha: 0, reg_lambda: 0.6536, subsample: 0.5 Table  S28 . The performance of XGBoost with/without temporal decoupling. "Precision", "Specificity" and "Recall" are calculated with HEDs as the class of interest. "F1 score" is the abbreviation for micro-F1 score.   Table  S29 . The cross feature comes from the Hadamard product of the original feature 1 vector and the original feature 2 vector. cross_fea_n_0=1-cross_fea_n.  Figure S1. The total number of papers published each year in the dataset. 
 Figure S2 . S2 Figure S2. The relationship between deposition method, anti-solvent treatment and the average PCE of the corresponding PSCs over time 
 Figure S5 . S5 Figure S5. After our feature engineering process, we tested the dataset using (a) Random Forest (RF) and (b) XGBoost to evaluate their performance on regression tasks. When compared to prior work, we assessed our model's performance, encompassing both RMSE (c) and R 2 (d). 
 Figure S6 . S6 Figure S6. Results of the Dtrees with training and testing datasets in the ratio of 4:1. (a) Accuracy of the Dtree between training and testing sets under different random state partitions. (b) The results on training sets and (c) the results on test sets. 
 Figure S7 . S7 Figure S7. Model performance after 100 iterations of ten-fold cross-validation. We recorded the average accuracy of each ten-fold cross validation for plotting purposes. (a) and (c) Bar chart of the accuracy with standard deviation, with numbers indicating the average accuracy. (b) and (d) Violin plot of the accuracy for each model. The black numbers represent the average accuracy. (a) and (b) consider the random factors of the model. (c) and (d) consider the random factors of the training data. 
 Figure S8 . S8 Figure S8. Confusion matrix of the XGBoost(a,d), LightGBM(b, e), and CatBoost(c, f) before (a, b, c) and after(d, e, f) data augmentation. Test size is 0.2. 
 Figure S9 . S9 Figure S9. The Bayesian optimization process for XGBoost(a, d), LightGBM(b, e), and CatBoost(c, f) in the experiments before(a, b, c) and after(d, e, f) data augmentation. Test size is 0.2. 
 Figure S10 . S10 Figure S10. Feature importance of XGBoost classification model before data augmentation. 
 Figure S11 . S11 Figure S11. Feature importance of CatBoost classification model before data augmentation. 
 Figure S12 . S12 Figure S12. SHAP analysis of the ML model (the models used were XGBoost (a), LightGBM (b), CatBoost (c)) before data augmentation. The length of stacked bars represents the relative contribution of each feature to a specific category. 
 Figure S13 . S13 Figure S13. SHAP analysis of the ML model (the models used were XGBoost (a), LightGBM (b), CatBoost (c)) after data augmentation. The length of stacked bars represents the relative contribution of each feature to a specific category. 
 Figure S14 . S14 Figure S14. The relationship between the accuracy of extended models and the dataset. (a) The maximum accuracy of models at different training size. (b-d) The accuracy of the XGBoost/LightGBM/CatBoost model under different random states and different amounts of training data. More detail can be found in TableS24-26. 
 Figure S15 . S15 Figure S15. The feature importance of the XGBoost model. (a) with temporal decoupling (b)without temporal decoupling. The ratio between the training and test sets is 9:1, and data augmentation has been employed. 
 Figure S16 . S16 Figure S16. The average absolute feature importance ranking is generated by SHAP library using the XGBoost model without time decoupling, and is arranged in descending order of importance. 
 
 
 
 Table S1 . S1 The relationship between PCE categories and ETL. ETL 0 1 2 All BCP 3 9 12 C60 0 6 0 6 PCBM 4 18 5 SnO 2 3 65 94 162 TiO 2 279 870 190 1339 ZnO 16 56 3 none 17 32 0 rare_ETL 9 76 53 138 All 331 1132 357 1820 rare_ETL : ['others', 'Zn 2 SnO 4 ', 'SnO2-Cl', 'Nb 2 O 5 ', 'CdS','c-TiO 2 ', 'SnO 2 -EDTAK', 'Bi-SnO 2 ', 'TiO 2 -SnO 2 ', 'TiO 2 -Cl', 'WOx','ZnSnO 3 ', 'Zn 2 Sn 3 O 8 ', 'PFN-2TNDI', 'In 2 O 3 ', 'ZnO-Li', 'graphe ne','PEIE', 'ZnO-Cs', 'H-WOx', 'SnO 2 QD', 'ZnO-PEG', 'NP-SnO 2 ', 'SG-SnO 2 ','MZO', 'PDI', 'Si-ZnSnO', 'PFN ITOkismina alsak mi','PEIE-LiQ', 'SnO 2 :C60-EDA', 'ZTO', 'C60-N-DPBI', 'SnO 2 -KCl','SnO 2 -RCQs', 'ZSO', 'TiZn 2 O 4 ', 'P123-ib', 'ZnS'] and so on 
 Table S2 . S2 The relationship between PCE categories and ETL_2. ZnOnanorod', 'mTiO 2 mZrO 2 ', 'Nb 2 O 5 ', 'mAl 2 O 3 -Cu:NiO', 'ZnO' rare_ETL2: 'PCBM-N-DPBI','CdSnanorod', 'Nb 2 O 5 ', 'NMBF-H','NMBF-Cl', All 331 1132 357 1820 other oxide：[''others', 'PEI', 'mZrO 2 ', 'ZnOnanocolu mn', 'NMBF-H dimer','mZnO', 'NMBF-Cl dimer', 'WO 3 ', 'C60-SAM', 'Al-ZnOnanorod', 'SAM','CdS', 'C3-SAM', 'mSnO 2 ', 'mSiO 2 ', 'PA-SAM', 'bis-PCBM','mTiO 2 -PMMA/PC BM', 'Au@SiO 2 ', 'porous- ZSO', 'ITIC', 'cZnO-nanowire','C60-ETA', 'bis-PCBM-DMC', 'NSGQDS', '2Dgraphene', 'PS', 'm-Al-ZnO','mZn 2 SnO 4 ', 'PCBB 2 CN 2 C 8 ', 'mZn 2 SnO 4 ', 'm- TiO 2 ', 'N-ZnOnanorod', 'mTiO 2 -BP', 'Msto', 'ZGO'], and so on. ETL_2 0 1 2 All C60 4 26 16 PCBA 0 2 5 7 PCBM 2 18 12 SnO 2 0 2 5 7 TiO 2 10 58 15 mAl 2 O 3 19 31 0 mTiO 2 185 478 93 756 None 101 466 179 746 other oxide 7 21 7 rare_ETL2 3 30 25 
 Table S3 . S3 The relationship between PCE categories and Perovskite. Br 0.35 ','(FA 0.76 MA 0. 21 Cs 0.03 ) 0.67 Pb(I 0.89 Br 0.11 ) 2.56 ','Cs 0.05 FA 0.7 9 MA 0.16 Pb(I 0.83 Br 0.17 ) 3 ', 'FA 0.9 Cs 0.1 PbI 3 ','FA 0.55 MA 0. 25 Cs 0.2 PbI 3 ', '(MA,FA)Pb(I, Br) 3 ', 'CsMAFAPbI Br ','PbI 2 ,FAI,MABr', 'Cs 0.05 FA 0.5 4 MA 0.41 Pb(I 0.98 Br 0.02 ) 3 -EAMA'] and so on. Perovskite 0 1 2 All Cs-FA 1 8 5 14 Cs-FA-MA 3 18 30 51 CsPbI 2 Br 0 13 0 13 CsPbI 3 10 27 1 38 FA-MA 0 5 22 27 MAPbI 3 207 693 105 1005 fuhe_perovskite 37 79 71 187 hyanion_pvs 56 216 17 289 hyperovskite 0 20 18 38 rareperovskite 17 53 88 158 All 331 1132 357 1820 hyperovskite:['(FAPbI 3 ) 0.85 (MAPbBr 3 ) 0.15 ','(FAPbI 3 ) x (MAPbBr 3 ) 1- x ','(FAPbI 3 ) 0.85 (MAPbBr 3 ) 0.15 ','(FAPbI 3 ) 0.85 (MAPbBr 3 )','(FAPbI 3 ) 0.85 (MAPbBr 3 ) 0.15 ',' MAPbI 3 -5AVAI'] hyanion_pvs: ['MAPbI 3 -xCl x ', 'MAPbI 3 xBr x '] MAPbI 3 : ['MAPbI 3 ', 'FAPbI 3 ', 'MAPbBr 3 '] rareperovskite:['others', 'Cs 0.05 (MA 0.17 FA 0.83 ) 0.98 Pb(I 0.83 Br 0.17 )', 'BA 0.05 (FA 0. 83 Cs 0. 17 ) 0. 95 Pb(I 0.8 Br 0.2 ) 3 ', 'BA 0.09 (FA 0 .83 Cs 0.17 ) 0. 91 Pb(I 0.6 Br 0.4 ) 3 ', '(FA 0.8 MA 0. 2 Sn 0.5 Pb 0.5 I 3 ) ', 'n-BAI-Cs 0.07 Rb 0.0 3 FA 0.765 MA 0.135 PbI 2.55 Br 0.45 -n-BAI'] and others which val ue_counts is 1. fuhe_perovskite: ['CsI, MAI, MABr, PbI 2 , PbBr 2 ','Cs 0.05 (MA 0. 17 FA 0.83 ) 0.98 Pb(I 0.8 3 Br 0.17 )', 'CH 3 NH 3 PbI 3 ','FAPbI 3 -xBr x ', 'PbI 2 ,FAI,MA I,MACl','Cs 0.1 (MA 0.1 5 FA 0.8 5 )Pb(I 0.85 Br) 3 ','Cs 0.05 ((MA 0.15 FA 0.85 ) (0.95) Pb(I 0.8 5 Br 0.15 ) 3 ','FA 0.75 MA 0. 15 Cs 0.1 Pb(I 2 ) 0.65 
 Table S4 . S4 The relationship between PCE categories and Deposition_procedure. Deposition_procedure 0 1 2 All one-step 211 685 297 1193 two-step 120 447 60 627 All 331 1132 357 1820 
 Table S5 . S5 The relationship between PCE categories and Precursor_solution. Precursor_solution 0 1 2 All DMF 214 622 44 880 DMF+DMSO 27 226 258 511 DMSO 11 59 6 76 DMSO+GBL 7 47 11 65 GBL 48 57 2 107 hybrid_precursor 19 72 32 123 none 5 49 4 58 All 331 1132 357 1820 ', 'DMF+NH 4 SCN', 'DMF+cyclohe xane', 'NEP', 'DMF+CNT','DMF+BAI', 'DMF+others', 'DMSO+GBL+HaHc', 'DMF+Al', 'DMF+4MSA','DMSO+GBL+CN', 'DMF+CuPc(t Bu) 4 ', 'DMF+DMSO +SnF 2 +pyrazi ne','MAAc',DMF+graphe ne', 'DMF+PEI', 'DMF+MACl', 'DMF+DMSO +2pyridylthioure a','DMAC', 'DMF+DMSO +NMP', 'others', 'DMF+DMSO +LiI', 'DMF+PCBM','DMF+PCBM+PEG', 'DMF+PAA', 'DMF+NH4I', 'GBL+DMSO', 'DMF+H3PO2','DMF+HAc', 'DMF+BMImI', 'DMF+DMSO +thiourea', 'DMF+PDM Surea'] hybrid_precursor: ['DMF+DMSO +IPA', 'DMA', 'DMF+GBL', 'DMF+HP', 'DMF+HI', 'DMF+H3P','DMF+DMSO+KI','DMSO+SnF 2 ','DMF+NMP','DMF+HCl','DMF+I PA','DMF+DMSO+others','DMF+HI+HBr','DMF+PVP','DMF+DMSO+PVP','D MF+thiourea','DMF+HBr','DMF+Li','DMF+PEG','NMP','DMF+TBP','DMSO+IP A','GBL+NMP','DMF+DMSO 0','DMF+NH 4 Cl','DMF+MCN','DMF+s-CNT 
 Table S6 . S6 The relationship between PCE categories and Anti_solvent_treatment. toluene 15 47 16 78 use_other 3 37 30 70 All 331 1132 357 1820 use_other: ['trifluorotoluene', 'anisole', 'IPA', 'ethyl acetate', 'chlorobenzene +toluene', 'ethylacetate', 'diethyl ethe', 'ethanol', 'ethyl acetate+hexane', 'ethyl ether', 'chlorobenzene 0', 'PCBM+chloro benzene', 'chlorobenzene +PF0', 'acetic acid', 'n-hexane', 'chlorobenzene +acetonitrile', 'ethoxyethane', 'chlorobenzene +DMSO', 'chlorobenzene +F+N2200', 'chlorobenzene +TOPO', 'methoxybenze ne', 'chlorobenzene +TPPO', 'diisopropyl ether'] Anti_solvent_treatment 0 1 2 All Chlorobenzene 20 156 154 330 diethyl ether 5 38 40 83 None 288 854 117 1259 
 Table S7 . S7 The relationship between PCE categories and Deposition_method. Deposition_method 0 1 2 All others 10 83 9 102 spin 206 632 222 1060 spin 2-3 31 167 119 317 spin-dip 79 218 7 304 vasp 5 32 0 37 All 331 1132 357 1820 Spin 2-3: spin 2-3 times in one step. others: ['others', 'CVD', 'spin-spray', 'spray', 'drip', 'evapspin', 'dipping', 'blowdry',  'bladedip', ' infiltratio n -dip', 'spin-OSA', 'electrod e position-dip', 'SVD', 'slot-die', 'printed', 'spin-OTA', 'spin-contact', 'spin-MAK', 'electrod e positiondip', 'spin 2-3dip', 'spin-electros pr ay', 'spin-TSA', 'sprayroll', 'evaporat ion-spin', 'dip-spin', 'MASP', 'PLD']     
 Table S8 . S8 The relationship between PCE categories and HTL. TFB', 'PFB', 'Cu 2 O', 'Co 3 O 4 ', 'CoPcNO 2 -Oph', 'PNBA', 'GO', 'V9no8'] and so on. HTL 0 1 2 All P3HT 34 40 1 75 PCBM 3 11 11 25 PTAA 12 40 24 76 none 61 62 0 123 others 68 159 30 257 spiro-OMeTAD 153 820 291 1264 All 331 1132 357 1820 others: ['CuSCN', 'P3CT', 'PEDOT', 'Without HTL', 'CuI', 'DEPT-SC', 'CBP', 'H1no1', 'CuPc', 'SWNT-PMMA', 'TPD', 'KTM3', 'OMeTPA-FA', 'PFO', 'PDTSTTz-4', 'PCBTDPP', 'PCDTBT', 'PTB-DCB21', 'Rno1', 'EH44', 'OMeTPA-TPA', 'PIF8-TAA', 'TTF-1', 'TSHBC', 'PDTSTTz', 'CuInS 2 /ZnS', 'NiO x ', 'TPA-TPM', 'DR3TBDTT', 'PF8-TAA', 'SWNT-polycarbonate', 'PCPDTBT', 'FDT', 'PDI', ' 
 Table S9 . S9 The relationship between PCE categories and HTL_additive. HTL_additive 0 1 2 All Li+TBP 156 718 220 1094 Li+TBP+FK 3 32 38 102 Li+TBP+FK 9 107 67 183 209 none 150 229 55 434 other_HTLadd 13 46 12 71 All 331 1132 357 1820 other_HTLadd: ['Li+2,6lutidie ne', 'TFSI+TBP', 'Li', 'TBP', 'H+TBP', 'Li+TBP+TF SI', 'Li+TBP+MY 11', 'Li+TBP+FK 269', 'F4TCNQ', 'EH44ox+TBP', 'lutidiene+TB P', 'Li+TBP+TP A-EABr', 'CuI', 'Li+D-TBP', 'D-TBP', 'Li+TBP+FK 210', 'Li+TBP+BP O', 'Li+TBP+PbI 2 ', 'CuSCN', 'H-TFSI+Et4N-TFSI', 'DIO', 'TPFPB', 'BuPyIm-TFSI', 'Li+TBP+TP A-PEABr', 'PVBI-TFSI', 'spiro(TFSI)2 +TBP', 'graphene', 'Li+PFPPY', 'Li+TBP+Ir', 'Li+TBP+Cu(BPCM)', 'HTFSI+Et4NTFSI+Al 2 O 3 '] 
 Table S10 . S10 The joint relationship between PCE categories and Deposition_procedure and Anti_solvent_treatment. Deposition_procedure Anti_solvent_treatment 0 1 2 All chlorobenzene 20 151 326 diethyl ether 3 40 80 one-step none 170 62 648 toluene 15 15 76 use_other 3 29 63 chlorobenzene 0 3 4 diethyl ether 2 0 3 two-step none 118 55 611 toluene 0 1 2 use_other 0 1 7 All 331 357 1820 
 Table S11 . S11 The joint relationship between PCE categories and Perovskite and Anti_solvent_treatment. All Cs-FA 1 331 1 1132 2 357 Cs-FA-MA 1 10 16 CsPbI 3 0 2 0 FA-MA 0 2 5 chlorobenzene MAPbI 3 12 82 27 fuhe_perovskite 6 29 38 hyanion_pvs 0 5 2 hyperovskite 0 13 9 rareperovskite 0 12 55 Cs-FA-MA 0 1 1 FA-MA 0 0 4 diethyl ether MAPbI 3 1 29 22 fuhe_perovskite 3 4 7 hyanion_pvs 1 2 1 rareperovskite 0 2 5 Cs-FA 0 5 1 Cs-FA-MA 1 6 8 CsPbI 2 Br 0 11 0 CsPbI 3 9 23 1 FA-MA 0 2 4 none MAPbI 3 183 530 42 fuhe_perovskite 24 33 15 hyanion_pvs 54 205 14 hyperovskite 0 4 7 rareperovskite 17 35 25 Cs-FA-MA 0 1 3 FA-MA 0 1 6 MAPbI 3 11 39 6 toluene fuhe_perovskite 3 3 0 hyanion_pvs 1 3 0 hyperovskite 0 0 1 Cs-FA 0 2 2 Cs-FA-MA 1 0 2 CsPbI 2 Br 0 2 0 CsPbI 3 1 2 0 FA-MA 0 0 3 use_other MAPbI 3 0 13 8 fuhe_perovskite 1 10 11 hyanion_pvs 0 1 0 Anti_solvent_treatment Perovskite hyperovskite rareperovskite 0 0 0 1 3 4 2 1 3 All S14 
 Table S12 . S12 The joint relationship between PCE categories and Perovskite and Deposition_procedure. Deposition_procedure Perovskite 0 1 2 All Cs-FA 1 8 5 Cs-FA-MA 3 18 25 CsPbI 2 Br 0 13 0 CsPbI 3 3 21 1 one-step FA-MA 0 5 20 MAPbI 3 119 310 89 fuhe_perovskite 26 62 62 hyanion_pvs 43 182 10 hyperovskite 0 20 12 rareperovskite 16 46 73 Cs-FA-MA 0 0 5 CsPbI 3 7 6 0 FA-MA 0 0 2 two-step MAPbI 3 88 383 16 fuhe_perovskite 11 17 9 hyanion_pvs 13 34 7 hyperovskite 0 0 6 rareperovskite 1 7 15 All 331 1132 357 
 Table S13 . S13 The joint relationship between PCE categories and ETL and ETL_2. PCBA 0 0 4 PCBM 0 8 5 SnO 2 0 1 0 TiO 2 TiO 2 10 53 15 mAl 2 O 3 14 28 0 mTiO 2 173 463 90 none 78 289 59 other oxide 1 8 2 rare_ETL2 3 14 15 C60 0 1 0 PCBA 0 1 0 PCBM 2 0 1 SnO 2 0 1 1 ZnO mAl 2 O 3 0 1 0 mTiO 2 1 3 0 none 8 38 1 other oxide 5 4 0 rare_ETL2 0 7 0 PCBM 0 1 0 TiO 2 0 5 0 mAl 2 O 3 3 0 0 none mTiO 2 9 5 0 none 4 16 0 other oxide 1 4 0 rare_ETL2 0 1 0 C60 0 1 1 ETL ETL_2 PCBA 0 0 1 1 2 1 All BCP C60 PCBM 3 0 9 3 12 1 C60 mAl 2 O 3 SnO2 0 0 1 0 0 4 rare_ETL none mAl 2 O 3 0 2 5 1 0 0 C60 mTiO 2 1 1 8 6 3 2 PCBM none none 3 6 8 60 2 41 other oxide other oxide 0 0 1 1 0 1 rare_ETL2 rare_ETL2 0 0 1 3 0 2 All C60 0 331 1 1132 0 357 PCBM 0 6 5 SnO2 mTiO 2 1 1 1 none 2 50 76 other oxide 0 3 4 rare_ETL2 0 4 8 C60 0 6 0 
 Table S14 . S14 The joint relationship between PCE categories and HTL and ETL_2. C60 rare_ETL2 0 2 2 23 0 23 All PCBM 1 331 2 1132 0 357 SnO 2 0 1 0 TiO 2 2 0 1 P3HT mAl 2 O 3 1 0 0 mTiO 2 22 8 0 none 5 23 0 other oxide 3 2 0 rare_ETL2 0 2 0 mAl 2 O 3 1 0 0 PCBM mTiO 2 1 1 0 none 1 10 11 C60 0 6 5 PCBM 0 2 1 TiO 2 2 4 2 PTAA mAl 2 O 3 1 0 0 mTiO 2 5 11 2 none 3 17 14 rare_ETL2 1 0 0 TiO 2 1 0 0 mAl 2 O 3 2 2 0 none mTiO 2 46 41 0 none 11 12 0 other oxide 1 6 0 rare_ETL2 0 1 0 C60 4 13 10 PCBM 0 1 0 TiO 2 0 3 0 others mAl 2 O 3 6 5 0 mTiO 2 38 97 10 none 20 36 8 rare_ETL2 0 4 2 C60 0 5 1 PCBA 0 2 5 PCBM 1 13 11 SnO 2 0 1 5 spiro-OMeTAD TiO 2 5 51 12 mAl 2 O 3 8 24 0 mTiO 2 73 320 81 none 61 368 146 HTL ETL_2 other oxide 0 3 1 13 2 7 All 
 Table S15 . S15 The joint relationship between PCE categories and HTL_additive and Perovskite. MAPbI 3 1 24 1 other_HTLadd fuhe_perovskite 6 2 3 hyanion_pvs 0 13 1 hyperovskite 0 0 1 HTL_additive Perovskite rareperovskite 0 6 1 4 2 0 All All Cs-FA 1 331 0 1132 3 357 Cs-FA-MA 1 4 19 CsPbI 2 Br 0 4 0 CsPbI 3 4 11 0 FA-MA 0 4 10 Li+TBP MAPbI 3 97 451 64 fuhe_perovskite 13 34 43 hyanion_pvs 32 176 15 hyperovskite 0 5 13 rareperovskite 8 29 53 MAPbI 3 3 29 1 Li+TBP+FK 102 fuhe_perovskite 0 2 2 hyanion_pvs 0 1 0 Cs-FA 0 4 1 Cs-FA-MA 1 4 3 CsPbI 2 Br 0 3 0 CsPbI 3 0 8 0 Li+TBP+FK 209 FA-MA 0 0 2 MAPbI 3 3 53 15 fuhe_perovskite 5 21 17 hyanion_pvs 0 2 1 hyperovskite 0 4 4 rareperovskite 0 8 24 Cs-FA 0 4 1 Cs-FA-MA 1 8 4 CsPbI 2 Br 0 5 0 CsPbI 3 6 8 1 none FA-MA 0 1 8 MAPbI 3 103 136 24 fuhe_perovskite 13 20 6 hyanion_pvs 24 24 0 hyperovskite 0 11 0 rareperovskite 3 12 11 Cs-FA-MA 0 2 4 CsPbI 2 Br 0 1 0 FA-MA 0 0 2 
 Table S16 . S16 The joint relationship between PCE categories and ETL_additive and Perovskite. C60 0 1 0 PCBM 2 0 0 fuhe_perovskite SnO 2 0 11 21 TiO 2 30 44 39 ZnO 0 5 0 none 0 2 0 rare_ETL 2 13 11 C60 0 3 0 Perovskite ETL PCBM 0 1 1 1 2 0 All BCP SnO 2 0 0 2 6 1 6 hyanion_pvs SnO 2 TiO 2 0 46 2 191 1 7 Cs-FA TiO 2 ZnO 1 1 2 1 1 0 ZnO none 0 6 0 5 1 0 rare_ETL rare_ETL 0 2 2 9 1 4 BCP SnO 2 0 0 1 1 3 6 hyperovskite PCBM TiO 2 0 0 3 19 2 11 Cs-FA-MA SnO 2 rare_ETL 1 0 1 0 13 1 TiO 2 PCBM 1 0 10 0 8 1 rare_ETL SnO 2 1 0 3 9 4 24 rareperovskite PCBM TiO 2 0 17 1 36 0 47 CsPbI 2 Br SnO 2 ZnO 0 0 6 1 0 0 TiO 2 rare_ETL 0 0 5 7 0 16 All ZnO 0 331 1 1132 0 357 PCBM 1 4 0 SnO 2 0 7 0 CsPbI 3 TiO 2 9 13 1 ZnO 0 1 0 rare_ETL 0 2 0 FA-MA BCP 0 0 5 PCBM 0 1 1 SnO 2 0 0 6 TiO 2 0 4 10 BCP 0 3 3 C60 0 2 0 PCBM 0 8 1 MAPbI 3 SnO 2 2 22 17 TiO 2 175 546 66 ZnO 15 47 2 none 11 26 0 rare_ETL 4 39 16 BCP 3 3 0 S19 
 Table S17 . S17 The joint relationship between PCE categories and Deposition_method and Perovskite. vasp 2 25 0 others 1 6 1 spin 20 39 32 fuhe_perovskite spin 2-3 7 32 38 spin-dip 9 1 0 vasp 0 1 0 others 0 16 3 spin 44 172 12 hyanion_pvs spin 2-3 3 19 2 spin-dip 6 3 0 vasp 3 6 0 hyperovskite spin 0 4 7 spin 2-3 0 16 11 others 0 5 1 spin 16 33 39 rareperovskite spin 2-3 0 12 47 spin-dip 1 3 1 All 331 1132 357 Perovskite Deposition_method 0 1 2 All Cs-FA others 0 2 0 spin 1 6 5 Cs-FA-MA others 0 1 1 spin 3 17 29 CsPbI 2 Br spin 0 10 0 spin 2-3 0 3 0 others 1 0 0 CsPbI 3 spin 3 27 1 spin-dip 6 0 0 FA-MA spin 0 5 22 others 8 53 3 spin 119 319 75 MAPbI 3 spin 2-3 21 85 21 spin-dip 57 211 6 
 Table S18 . S18 The joint relationship between PCE categories and Precursor_solution and Perovskite. Precursor_solution Perovskite 0 1 2 All Cs-FA 0 2 0 Cs-FA-MA 0 1 1 CsPbI 3 7 2 1 DMF FA-MA 0 2 2 MAPbI 3 129 409 19 fuhe_perovskite 17 10 2 hyanion_pvs 50 179 10 rareperovskite 11 17 9 Cs-FA 1 4 5 Cs-FA-MA 3 15 27 CsPbI 2 Br 0 4 0 CsPbI 3 0 13 0 FA-MA 0 3 19 MAPbI 3 12 97 66 fuhe_perovskite 8 44 61 
 Table S23 . S23 Feature importance of LightGBM classification model before data augmentation."split" reflects how often a feature is used for data partitioning, while "gain" quantifies the actual improvement in model performance resulting from that feature's splits. feature split gain cross_fea80_0 798 5.819049 cross_fea28_1 775 3.097369 cross_fea26_0 598 3.005758 cross_fea76_0 1422 3.004022 Year_2013 1482 2.443449 Anti_solvent_treatment_chlorobenzene 1808 2.172161 Year_2018 1117 2.160426 Year_2016 1632 2.130561 cross_fea12_0 437 2.102877 ETL_2_PCBA 1314 1.979595 cross_fea291_0 1414 1.970661 Year_2015 1867 1.960064 cross_fea70_1 1159 1.862034 cross_fea294_0 1707 1.833108 Precursor_solution_no 1514 1.803947 Year_2014 1535 1.742783 cross_fea71_1 932 1.698537 cross_fea71_0 1120 1.55312 cross_fea11_1 1380 1.504971 Anti_solvent_treatment_no 1310 1.448406 
 Table S24 . S24 The maximum accuracy of the XGBoost and the corresponding random state under different test set sizes. The total time expenditure for grid search was 841.577 seconds (run 400 times). Test size Max accuracy Random_state 0.30 07692 147 0.25 0.7780 158 0.20 0.7774 95 0.15 0.8066 159 0.10 0.8306 166 
 Table S25 . S25 The maximum accuracy of the LightGBM and the corresponding random state under different test set sizes. The total time expenditure for grid search was 2068.851 seconds (run 400 times). Test size Max accuracy Random_state 0.30 0.7637 147 0.25 0.7824 95 0.20 0.7939 158 0.15 0.7919 151 0.10 0.8142 166 
 Table S26 . S26 The maximum accuracy of the CatBoost and the corresponding random state under different test set sizes. The total time expenditure for grid search was 18994.99 seconds (run 400 times). Test size Max accuracy Random_state 0.30 0.7839 147 0.25 0.7956 158 0.20 0.7912 95 0.15 0.7893 151 0.10 0.8251 382 
 "CV_mean" represents the sum of the accuracy of each fold in 10-fold cross validation, which is then averaged. temporal Accuracy Precision Recall F1 score Specificity CV_mean decoupling without 0.8571 0.9394 0.8158 0.8571 0.9861 0.8310 with 0.8351 0.9688 0.8158 0.8351 0.9931 0.8327 
 Table S30 . S30 The XGBoost model predicts high-efficiency PSCs devices. Sample ID ETL ETL_2 Perovskite Deposition procedure Deposition method Anti_solvent treatment Precursor solution HTL HTL_ additive PCE (%) 3 BCP C60 Cs-FA one-step Doctor blading no DMF+ DMSO PTAA no 22 4 SnO 2 no MAPbI 3 one-step spin Diethyl ether DMF+ DMSO Spiro-MeOTAD Li+ TBP 25.3 6 SnO 2 no MAPbI 3 two-step spin no DMF+ DMSO Spiro-MeOTAD Li+ TBP 23.9 117 TiO 2 no FA-MA one-step spin Diethyl ether DMF+ DMSO Spiro-MeOTAD Li+ TBP 25.18 136 BCP C60 Cs-FA one-step spin 2-3 Ethyl acetate DMF+ DMSO spiro-OMeTAD no 17.3 137 BCP C60 Cs-FA one-step spin 2-3 Ethyl acetate DMF+ DMSO spiro-OMeTAD no 37.5 138 BCP C60 Cs-FA one-step spin 2-3 Ethyl acetate DMF+ DMSO spiro-OMeTAD no 35 139 BCP C60 Cs-FA one-step spin 2-3 Ethyl acetate DMF+ DMSO spiro-OMeTAD no 30.4 140 BCP C60 Cs-FA one-step spin 2-3 chlorobenzene DMF+ DMSO spiro-OMeTAD no 18.7 141 BCP C60 Cs-FA one-step spin 2-3 chlorobenzene DMF+ DMSO spiro-OMeTAD no 33.7 142 BCP C60 Cs-FA one-step spin 2-3 chlorobenzene DMF+ DMSO spiro-OMeTAD no 29.2 143 BCP C60 Cs-FA one-step spin 2-3 chlorobenzene DMF+ DMSO spiro-OMeTAD no 25.4 Cross feature Original feature 1 Original feature 2 cross_fea1 ETL_TiO 2 ETL_2_TiO 2 cross_fea2 ETL_TiO 2 ETL_2_TiO 2 cross_fea3 ETL_TiO 2 ETL_2_ mAl 2 O 3 cross_fea4 ETL_TiO 2 ETL_2_C60/PCBM/PCBA cross_fea5 ETL_BCP ETL_2_C60 cross_fea6 ETL_TiO 2 ETL_2_rare_ETL2 cross_fea8 ETL_rare_ETL ETL_2_no cross_fea9 ETL_PCBM ETL_2_PCBA/BCP cross_fea11 ETL_SnO 2 ETL_2_no cross_fea12 ETL_SnO 2 ETL_2_rare_ETL2 cross_fea13 ETL_TiO 2 ETL_2_no cross_fea20 ETL_TiO 2 Perovskite_MAPbI 3 cross_fea21 ETL_SnO 2 Perovskite_MAPbI 3 cross_fea22 ETL_rare_ETL Perovskite_MAPbI 3
Paper #: 13964_2
Title: Temporal Decoupling-Based Machine Learning Framework for Precise Efficiency Prediction in Perovskite Solar Cells
The rapid and accurate identification of potential high-efficiency design strategies for perovskite solar cells (PSCs) is of paramount importance in advancing their development and commercialization. However, the application of machine learning (ML) algorithms in this field is hindered by unstable PSC data sets (e.g., time-related noise and data imbalance). Here, we introduce a ML framework specifically tailored for temporal decoupling through feature engineering and uncertainty modeling (noise processing techniques) to accurately predict the efficiency of PSCs. In our framework, we utilize one-hot encoding and feature fusion methods to extract features from a shared data set encompassing perovskite material, processing, and PSC architecture. After temporal decoupling, our ML model shows an outstanding precision of 96.88% and a specificity of 99.3% for high-efficiency devices and is successfully applied to predict PSC efficiencies in the period between 2021 and 2023. This temporal decoupling ML framework also reveals hidden relationships between features and efficiency through cross-feature analysis. Our work demonstrates the potential of ML for predicting performance and elucidates the associated mechanisms, accelerating PSC commercialization and reducing costs.
INTRODUCTION Extensive research on perovskite materials and devices has driven the application of data-driven machine learning (ML) methods in searching for and developing counterparts with comparable or even higher efficiencies.  [1] [2] [3] [4]  ML, which involves establishing end-to-end mappings between descriptors (features) and target values (labels) from extensive data sets, has demonstrated computational accuracy on par with traditional high-throughput methods such as density functional theory (DFT) while requiring no knowledge of quantum chemistry theories and lower time costs.  [5] [6] [7]  However, research on the prediction of perovskite solar cell (PSC) performance through ML methods remains limited, primarily due to the challenge of acquiring low-noise data.  [8] [9] [10] [11]  Furthermore, in the current stage, most studies construct ML models based on data sets with various descriptors. Such biased data sets have limited applicability and fail to uncover generalized patterns. The power conversion efficiencies (PCEs) of PSCs are strongly dependent on device architectures and fabrication processes. By choosing proper parameters for ML modeling, the underlying working mechanisms responsible for efficient photoelectric conversion can be identified.  9, [12] [13] [14] [15]  Li et al.  8  established ML models to predict both perovskite bandgaps and corresponding PCE by considering the composition of perovskite and the energy potential barriers between perovskite and charge transport layers. Their method enabled them to more reasonably screen charge transport layers that match the bandgap difference based on the bandgap of the perovskite. Cai et al.  9  conducted their research based on a similar method, revealing the asymmetric relationship between the Sn-Pb ratio and perovskite band gap. Subsequently, they introduced timerelated factors to predict the optimal ratio for the bandgap of perovskite. Nevertheless, microscale features still rely heavily on time-consuming high-throughput calculations and cumbersome experimental characterizations. Odabasi and Yıldırım  16  extensively analyzed PSC information directly related to device architecture and fabrication processes from previous studies but preferred to use statistical analysis to extract key factors affecting the PCE since they did not sufficiently optimize feature engineering and ML models. The lack of ML research analyzing features based on structure-process relationships always results in insufficient model expressiveness and poor interpretability. In addition, the reported device efficiencies demonstrated a significant correlation with the year of publication.  17  Due to improvements in the material process, the film quality improves over time, and devices with the same structure commonly exhibit variable efficiencies in their early stages. Likewise, the optimal electron transport layer (ETL) and hole transport layer (HTL) materials in contact with the perovskite absorption layer vary with time.  18  Consequently, original data sets inherently contain substantial time-related noise, and improper noise handling can bias ML models. Another limitation of ML models is their inability to handle "unseen variables", 19 so models trained using previous temporal information struggle to predict the efficiency of future PSCs, which is a serious concern for model generalizability. Although deep learning networks such as recurrent neural networks and long short-term memory networks demonstrate excellent performance in time-series forecasting, they require highquality and balanced input with time steps and may still fail in this field.  20, 21  Given that the temporal evolution of the PCE is inherently correlated with changes in material processing and device architecture, it is possible to decouple temporal information via proper feature fusion of the details of material processing and device architecture.  22  The concept of temporal decoupling involves substituting the time features with crossfeatures that combine the device architectures and material processing methods, which aims to achieve the performance of the temporal decoupling model comparable to that of preserving time features. In that case, cross-feature engineering, as a form of feature fusion, can significantly enhance the robustness and expressiveness of models.  23, 24  In this study, we initially developed a precise ML framework by performing temporal decoupling to predict PSC efficiency. Temporal decoupling is enabled through cross-feature engineering and uncertainty modeling. Cross-feature engineering involves the combination of nine easy-to-find descriptors, enhancing the expressiveness of the data, while uncertainty modeling addresses noise in such imbalanced data sets, yielding a substantial enhancement in model generalizability. The applied XGBoost model accurately predicts the efficiency distribution after temporal decoupling, enabling an intuitive assessment of device-level feature contributions. Our supervised ML model demonstrates exceptional accuracy and precision in predicting PCE across historical data sets (2013-2020) and showcases generalizability to new data (2021-2023). The importance of the features, as analyzed by Shapley additive explanations (SHAP) after temporal decoupling, provides deeper insights into achieving high-efficiency devices (HEDs). These findings underscore the substantial advantage of employing ML techniques in advancing HED development. 
 RESULTS AND DISCUSSION 2.1. Machine Learning Framework. The high-performance flowchart for predicting the PCE is depicted in Figure  1a , which includes four key components: data preparation, uncertainty modeling, model selection, and model development. In the initial preparation phase (Figure  1b ), the input data undergo validation and visualization, followed by feature extraction and merging based on insights from device fabrication. Visual tools and domain expertise are employed to optimize feature fusion and generate cross-features. The uncertainty modeling phase assesses the impact of data noise and information loss. As illustrated in Figure  1c , the synthetic minority oversampling technique (SMOTE) is employed to balance sample quantities between classes, and k-fold crossvalidation (CV) is also applied during model training to address diverse data distributions for enhanced feature learning. The model selection phase, depicted in Figure  1d , focuses on computational efficiency, initially using common base models. The evaluation extends to incorporate classical algorithms with similarities to the optimal base model, and the best-performing model is chosen based on accuracy and precision metrics. In the final model development phase, shown in Figure  1e , pre-and postdecoupling temporal information models are assessed for potential interchangeability. The SHAP library elucidates the potential mechanism impacting PSCs, providing insights for HED development. The robustness and usability of the framework are validated using data from subsequent devices. 2.2. Feature Engineering. The data set spanning from 2013 to 2020, sourced from the literature,  25  is utilized for model training, while the data set from 2021 to 2023 is used for validation.  26  The experimental data set contains 1820 PSC samples with 10 descriptors, as shown in Tables S1-S9, including the publication year (from 2013 to 2020), perovskite materials, material categories for the ETL and second layer of the ETL (ETL_2), perovskite precursor solvent, deposition method and procedure, antisolvent treatment, HTL materials, and additives. These descriptors are essential and commonly used parameters in the PSC fabrication process, with the target being the PCE. We acknowledge that there are many features that align more closely with domain knowledge; 27 however, we only use the aforementioned features based on our stringent requirements for ease of collection and universality. In this analysis, we conducted a reassessment of feature correlations, focusing on four classical material features (ETL, perovskite precursor solvent, HTL, and HTL additives), as depicted in Figure  2a-d . The bubble size represents the The Journal of Physical Chemistry C number of corresponding publications. Across the four feature plots, a discernible trend emerges, indicating an overall increase in the PCE over time. Simultaneously, it becomes evident that specific features exhibit correlations with time. Figure  2  shows the evolution of the ETL materials over time. For instance, TiO 2 was predominantly used initially, while SnO 2 has gained popularity since 2016, demonstrating a higher overall efficiency than that of the other two types applied in PSCs. Figure  2b  illustrates the evolution of perovskite precursor solvents from the original use of DMF to the current preference for DMF and DMSO mixtures (DMF + DMSO), which have become the solvent of choice for achieving high efficiency. Similarly, both the HTL materials in Figure  2c  and the HTL additive in Figure  2d  undergo changes over time. The relationships between the device efficiency and deposition methods, along with the use of antisolvent treatments, are shown as a function of time (Figure  S2 ). Figure  S2a  shows a comparative analysis of the four deposition methods. Initially, spin-coating methods were not superior. However, since the development and adoption of antisolvent treatments such as chlorobenzene and diethyl ether in 2015 (Figure  S2b ), devices based on spin-coating methods have exhibited higher PCE than those based on alternative methods. According to these results, we posit a correlation between time and efficiency, attributed to the continuous progression of materials and the optimization of processing methods, along with the amalgamation of certain features. Furthermore, certain material or process descriptors exhibit lower frequencies, as observed in the ETL category (refer to Table  S1 ). For instance, the use of C60 appears only six times, which may not be consistent with reality. Additionally, Figure  S2a  illustrates that in 2019, only the spin method was employed, indicating information loss from other deposition methods. This observation underscores the challenges inherent in constructing extensive data sets and the inherent difficulty in mitigating data noise. Therefore, compared to the tedious and complex data collection process, high-throughput experimental methods may be better used for ML, and experimenters could The Journal of Physical Chemistry C more conveniently conduct experimental verification and analysis.  28  Figure  2e  illustrates the distribution for the target values of the PCE, which is fitted with a Gaussian function. We employ the μ ± σ strategy to bin the data into categories of "low efficiency", "medium efficiency", and "high efficiency", encoded numerically as 0, 1, and 2, respectively, in ascending order. The quantities within each category are presented in Figure  2f . To obtain richer semantic information from the data set, we employ cross-feature processing involving combinations of different features to create new features, as outlined in Tables  S10-18 . In Tables S10-S12, we combine the ETL, perovskite component, and HTL features, taking into account the band alignment of the respective materials concerning charge carrier transport at the corresponding heterostructures. The combination of ETLs with ETLs in Table  S13  is associated with hole transport blocking or enhanced electron extraction. Considering the crystallization and film formation of the perovskite layer, features related to the perovskite components, perovskite solution, antisolvent treatment, deposition method, and deposition procedure are pairwise combined in Tables S14-S18. To better understand the impact of cross-features and time features on data points, we employ t-distributed stochastic neighbor embedding (t-SNE) to project the feature space into two dimensions, tSNE_1 and tSNE_2, to visualize the sample distribution.  29  Figure  S3  illustrates the feature distributions with (temporal coupling) and without time features (temporal decoupling). When contrasting data sets lacking cross-features (Figure  S3b, d ) with those incorporating cross-features (Figure  S3a, c ), the latter exhibit a greater number of data cluster centers and increased dispersion between different categories. This suggests an amplified expressiveness of the data when cross-features are introduced. Within the "with cross-features" data set, the features after temporal decoupling (Figure  S3a ) lead class 1 to have more data cluster centers and greater dispersion than did the features before temporal coupling (Figure  S3c ) but have a less effect on class 2, so the time features mainly affect class 1. Consequently, the integration of cross-features enhances the model's ability to apprehend fundamental structures and relationships within the data, especially in scenarios of temporal decoupling (Figure  S3a, d ). The introduction of cross-feature interactions to generate new features increases the complexity of the feature space in ML. However, the process of feature representation, which involves one-hot encoding followed by postfeature selection, often leads to information loss. Considering the numerous features, we adopt a direct and nonprior way to objectively evaluate the impact of feature selection. Initially, a ML model is trained, followed by the direct retention of all features with an importance higher than the average. Subsequently, the retraining process involves the filtered features, and the final evaluation of model performance is conducted separately on the test set. By introducing random factors into the model and training data with different training-test splits and repeating the above steps 500 times, the accuracy of the model before and after feature screening and the fluctuations caused by random factors are compared, as depicted in Figure  S4 . The XGBoost algorithm, which has been widely used in the field of predicting the properties of PSCs, was chosen for this experiment.  10, 30  XGBoost extends the cost function to a second-order Taylor expansion based on the traditional gradient boosting tree and adds a regularization term to penalize model complexity. As a result, XGBoost enhances its generalization ability while learning more details (more details about XGBoost can be found in Note S1.2).  31  "The disturbance of the model" and "the disturbance of the training data" are implemented by   S19  and S20 . 
 The Journal of Physical Chemistry C initializing the ML model and partitioning the training-test sets in different ways, respectively. Additionally, the influence of the training data size is investigated, revealing that larger data sizes (Figure  S4a, c ) exhibit greater accuracy than smaller ones (Figure  S4b, d ) before feature selection (BFS). Across all the groups, the accuracy of the model after feature selection (AFS) does not surpass that of BFS, suggesting that excluding features is unnecessary in this case. Moreover, it is difficult to filter features reasonably because a common approach is to remove features with high correlation, but the thresholds are often subjective.  2, 25, 32, 33  Furthermore, the total length of features after concatenating cross-features is still significantly smaller than the sample size (180/1820 < 1/10), rendering traditional feature selection less imperative in ML approaches.  19, 34  In light of these facts, we propose employing balancing regularization techniques to mitigate the influence of noncritical features to avoid the risk of overfitting and minimize the impact of uncertain factors (more details of our experiment can be seen in Note S1).  19, 35  2.3. Model Selection. As shown by "the disturbance of the model" in Figure  S4a , randomness in the model induces significant fluctuations in performance, implying that model optimization with time features is essential for assessing the feasibility of temporal decoupling. In the context of the initial continuous target values, regression prediction tasks are undertaken using the random forest (RF) model (Figure  S5a ) and the XGBoost model (Figure  S5b ).  [36] [37] [38]  Both models demonstrate a root-mean-square error (RMSE) of 2.80% and an R-squared (R 2 ) of 0.7 between the predicted and actual values, indicating a 26.51% decrease (Figure  S5c ) and a 62.79% increase (Figure  S5d ) compared to previous work. However, due to the R 2 below 0.8, this study shifts its emphasis from regression to classification. Figure  S6  compares the model performance fluctuations between the training and test sets under various data partitioning schemes. Across different training data scenarios, the model exhibits a maximum accuracy of 75% and a minimum accuracy of 60% on the test set, indicating limited feature generalizability. To mitigate the impact of occasional optimal test set results on the generalization ability and stabilize the performance of the model, k-fold CV (here, k = 10) is implemented, allowing the algorithm to learn from diverse data distribution scenarios and reduce overfitting.  19  To select a base model with optimal noise resistance, we consider a comprehensive model search space ("no free lunch" theorem),  39  which includes logistic regression (LR), linear discriminant analysis (LDA), support vector machines (SVC), k-nearest neighbor clustering (KNN), neural networks (MLP), random forest (RF), extra trees (Etrees), and the boosting tree algorithms (AdaBoost, XGBoost).  2, 25, [40] [41] [42] [43]  80% of the data is allocated for model training. Following hyperparameter tuning, models are evaluated on the hold-out validation set, and the final performance assessment is a weighted average of each validation set's performance. Figure  3  illustrates the performance of each model. In Figure  3a , it is evident that all types of learners exhibit excellent performance, with the simplest ML algorithm, LR, achieving an average accuracy of up to 72%, highlighting the enhanced relationship between the target and features. The average accuracy of the decision tree (Dtree) in 10-fold CV stands at only 66%, which is significantly lower than the best performance of 75% in Figure  S6c , emphasizing the challenge of generalizing occasional optimal values to other data sets. In Figure  3b , the upper quartile and median accuracy of the XGBoost model surpass those of the other models, suggesting its suitability for PSC data. This could be attributed to its gradual learning of residuals between the target and predicted values, along with the benefits from regularization to avoid overfitting, resulting in steadily improving performance. This idea has been widely applied in the field of deep learning.  [44] [45] [46]  To further test the robustness and credibility of the model, we investigate the impact of uncertain factors on the model and training data through 100 iterations of 10-fold CV, as summarized in Figure  S7 . In both "disturbance of the model" (Figure  S7a, b ) and "disturbance of the training data" (Figure  S7c, d ), XGBoost maintains the highest average accuracy with a very small standard deviation, indicating excellent robustness while sustaining optimal performance. To expand the algorithm space, the ensemble-boosting tree learning algorithms LightGBM and CatBoost are incorporated, which also offer the ability to adjust sample weights and employ regularization techniques.  47, 48  The remaining 20% of the data is reserved for further testing. Considering the trade-off between the time complexity in high-dimensional spaces and the global optimal solution, the Bayesian optimization (BO) algorithm is employed for hyperparameter tuning.  [49] [50] [51] [52]  The bounds of the hyperparameter space are dynamically adjusted through a progressive learning process, gradually converging to a smaller region of optimal values. As shown in Figure  3c , the three models (with the best hyperparameters detailed in Table  S21 ) exhibit a similar trend with relatively lower precision and higher specificity (Table  S19 ) when considering HEDs as the class of interest. This is attributed to the influence of the imbalanced data set, which causes the models to overly focus on medium-efficiency devices. To address the issue of imbalanced data sets (Figure  2f ), it is decided to oversample the minority class samples, thereby balancing the mediumefficiency samples. New samples are generated by interpolating around selected minority samples based on spatial relationships. These generated samples retain similar characteristics to the original samples while mitigating overfitting.  53  The data used for training and testing remain unchanged, and the SMOTE algorithm is employed to augment the training data. Subsequent experiments after data augmentation (Figure  3d ) show that all models demonstrate higher average accuracy (with the best hyperparameters detailed in Tables  S21  and S22 , respectively), exceeding 85% in 10-fold CV, compared to that before optimization, as shown in Figure  3c . Furthermore, a slight increase in model accuracy is observed, along with a significant improvement in precision, suggesting an enhanced ability of the model to identify HEDs. We conduct an analysis on the relationship between different train-test splitting ratios, random states, and model performances to determine whether we have maximized the utilization of the data. In Figure  S14a ,b, we observe that increasing the amount of training data leads to an enhancement in model accuracy, facilitating the model's ability to learn general features. Specifically, in Figure  S14a , the model consistently demonstrates improved performance as the volume of training data increases, with XGBoost exhibiting particularly outstanding results with higher maximum accuracy, especially when the training size exceeds 0.8. Furthermore, in terms of the algorithm's total runtime, it is noteworthy that XGBoost exhibits the shortest time of 841.58 s, as detailed in Tables  S24-S26 . Undoubtedly, based on its superior perform- The Journal of Physical Chemistry C ance and efficiency, XGBoost has emerged as the most suitable choice for model development. 
 Model Development. Before delving into the performance analysis without temporal information, we further examined the feature importance using the XGBoost model, as illustrated in Figure  4a . The top 20 important features for each model, from the analysis of local feature importance (based on the model), presented in Figures  S10  and S11  and Table  S23 , include years as time features but exhibit notable disparities in their rankings. For instance, in the XGBoost model (Figure  4a ), the temporal information "Year_2020" ranks first, while "Year_2017" ranks second, but the situation is different between LightGBM and CatBoost. The top four features in LightGBM are cross-features, while the top feature in CatBoost is "Year_2017". This discrepancy in the ranking of features is further observed through the SHAP library, which provides global interpretability of features (Figures  S12  and S13 ). This finding underscores the unstable impact that time features have on the prediction of PSC device performance. The disparities in time feature importance across models highlight the complexity and variability in the strong influence of timerelated factors on the predictive models. Figure  4b  presents a comparison of the test-size-dependent performance of XGBoost concerning the decoupling of time features. In the test size range of 0.1-0.3, the model without temporal information exhibits slightly lower performance than the original model. The decreased accuracy is consistent with the high importance of the time features, as resolved in Figure  4a . To evaluate the substitutability of the model without time features, further optimization is deemed necessary. Given the    S28 . 
 The Journal of Physical Chemistry C positive correlation between model performance and training data volume (Figure  S14a ) and data augmentation for further expansion of training data, we allocate 90% of the data for training and implement data augmentation on the training set for in-depth analysis. BO is employed to find the optimal hyperparameters, followed by a comparison on the test set. 2.5. Model Interpretability. As depicted in Figure  5b , the model retaining time features directs its attention more toward devices with medium and low efficiencies (the best hyperparameters are detailed in Table  S27 ). Whether the feature importance analysis is based on the XGBoost model itself (Figure  S15b ) or obtained in conjunction with SHAP, the features of significant importance still include slightly earlier time features associated with the large number of devices during that period (Figure  S1 ). Although decoupling time features reduces the model's accuracy slightly to 83.51% (Figure  5a ) compared to 85.71% with time features (Figure  5b ), the precision and specificity of the model for HEDs (class 2) increase to 96.88 and 99.3% (Figure  5a ), respectively, compared to 93.94 and 98.6% without temporal decoupling (Figure  5b ). It indicates that while time features enhance accuracy to some extent, they mislead the model's search for HEDs. This finding also suggests that direct time information can be effectively replaced by our feature engineering approach in this work. Our model with temporal decoupling also achieved a 25.61% improvement in accuracy (83.51% vs 66.48%) and a 40.22% increase in precision (96.88% vs 69.09%) compared to previous studies.  25  It is worth noting that in the optimized XGBoost model hyperparameters, "regalpha" and "reg_lambda" penalize the complexity of the model by controlling the absolute value of the weights and the square of the weights, respectively.  54  It is generally believed that the former promotes weights toward 0, while the latter promotes weights toward smaller values.  55, 56  As shown in Table  S27 , "reg_lambda" is greater than 0, while "reg-alpha" is equal to 0, reflecting the suppression rather than elimination effect of regularization techniques on weights. This also proves the nonnecessity of feature removal previously, and the same conclusion can be obtained from Tables  S21  and S22 . We further validate our developed model using a new data set with data from 2021 to 2023, as shown in Figure  5c . The HEDs we identified are truly all HEDs. Although one medium-efficiency device is mislabeled as having high efficiency, it shares the same structural and processing characteristics as most HEDs, leading to potential misclassification (Table  S29 ). A robust end-to-end predictive model yields enhanced insights into the intricate relationships and potential mechanisms within PSCs. To obtain a global feature importance analysis, we conduct interpretable analysis on the optimized XGBoost using the SHAP library in order to determine the features that have a significant impact on the model after temporal decoupling, as shown in Figure  5d, e . Figure  5d  illustrates the ranking of average absolute feature importance across all categories with temporal decoupling. In this context, HTL_additive_Li + TBP [abbreviation of the additive bis (trifluoromethane) sulfonimide lithium salt (Li-TFSI) and 4-tert-butylpyridine (TBP) in Spiro-MeOTAD], specifically the Li + TBP additive in the HTL (HTL_addi-tive_Li + TBP), emerges as the most influential contributor. Within the current n-i-p structured devices, Spiro-MeOTAD serves as the most commonly used hole transport layer, with Li + TBP serving as an effective additive to promote Spiro-MeOTAD oxidation.  57  The role of the HTL additive Li + TBP becomes paramount due to its crucial function in enhancing the hole collection efficiency and transport.  58  However, the hygroscopic nature of Li salt poses degradation risks to the perovskite.  59  Despite the ongoing development of alternative additives to replace Li + TBP, its presence remains a critical determinant of efficiency. Further analysis of the features important for HEDs (class 2) in Figure  5e  reveals that the precursor solution containing a mixture of DMF and DMSO (precursor_solution_DMF + DMSO) exerts the most significant influence, even surpassing HTL_additive_Li + TBP. This suggests that the precursor solution may influence the dynamics of perovskite crystalline growth, thereby potentially determining film quality, consistent with the current report.  60  Within the perovskite precursor solution, the importance of precursor_solution_DMF + DMSO exceeds that of precursor_solution_DMF, indicating the crucial role of DMSO in forming high-quality perovskites. This is hypothesized to be associated with the strong coordination of DMSO with PbI 2 and ammonium salts, forming intermediates conducive to retarding the crystalline growth of perovskite.  61  Therefore, under the condition of temporal decoupling, we identify that the primary factors influencing the PCE can be attributed to the optimization of the properties of the perovskite layer and HTL. In contrast, Figure  4a  (which shows feature importance using a model without temporal decoupling) indicates that time features such as "Year_2020" and "Year_2017" are ranked as the most important. However, Figure  5d ,e reveals the core features most closely related to efficiency, providing deeper insights into the fundamental factors driving efficiency improvements. This conclusion aligns with earlier modeling efforts, wherein a simplistic description of perovskite properties and the HTL yielded relatively accurate PCE predictions.  62, 63  Therefore, through extensive data-driven analysis, we have unveiled consistent trends in advancing high-efficiency PSCs research with the assistance of high-performance ML models under the condition of time decoupling. 
 CONCLUSIONS In summary, we have optimized the model's performance from both the feature and model perspectives. In this regard, temporal decoupling relies on cross-features that enhance sample similarity and uncertainty modeling that balances noise and information loss. We have also addressed challenges stemming from noisy and imbalanced data sets, which are common in most experimental data. In our model construction, we prioritized balancing the model's regularization capabilities rather than removing redundant features (i.e., feature importance selection). This approach differs from traditional methods because it is difficult to evaluate model performance in the presence of unstable data sets. The Bayesian-optimized XGBoost model, utilizing only nine easily accessible features, achieved an accuracy of 83.51%, a precision of 96.88%, and a specificity of 99.3% after decoupling the time features. The perovskite layer and HTL processing have been resolved to be the most crucial factors influencing the PCE based on SHAP library-based feature importance analysis. Finally, we have validated the optimized model on PSCs from 2021 to 2023, where all HEDs were successfully identified as having high efficiency. Our ML framework significantly improves the robustness and generalizability of the model, demonstrating its exceptional ability to predict material  Figure 1 . 1 Figure 1. Schematic diagram of the efficient machine learning process. (a) Flowchart of the machine learning process. (b) Preparation, (c) uncertainty modeling, (d) model selection, and (e) model development. 
 Figure 2 . 2 Figure 2. Data set analysis. The relationship between partial feature descriptors [(a) ETL, (b) Precursor_solution, (c) HTL, and (d) HTL_additive] and the average PCE of the corresponding PSCs over time. (e) Distribution of the PCE and binning boundaries (dotted lines). The symbols μ and σ are the mean and standard deviation of the fitting normal distribution, respectively. (f) Corresponding number of each category. 
 Figure 3 . 3 Figure 3. Performance of models based on time-coupling features. (a,b) Base models. (a) Bar chart of the accuracy with a standard deviation, with numbers indicating the average accuracy. (b) Violin plot of the accuracy. The black numbers represent the average accuracy. (c,d) Extended models on the data set before data augmentation (c) and after data augmentation (d). DA, data augmentation. The precision and specificity are calculated with HEDs as the class of interest. CV_mean represents the sum of the accuracy of each fold in 10-fold cross-validation, which is then averaged. More details can be found in TablesS19 and S20. 
 Figure 4 . 4 Figure 4. Impact of time features on the model. (a) Top 10 most important features (based on the model) of the XGBoost model before data augmentation (more details can be found in Figure S10). (b) Maximum accuracy of XGBoost before and after temporal decoupling with different test sizes. 
 Figure 5 . 5 Figure 5. Confusion matrix of XGBoost (a) with temporal decoupling, (b) without temporal decoupling, and (c) with temporal decoupling for the latest PSCs. (d,e) The feature importance ranking is generated by the SHAP library (the model used is XGBoost with temporal decoupling), and the features are arranged in descending order of importance. (d) Ranking of average absolute feature importance for all categories. The length of each color bar represents the absolute importance of the corresponding category. (e) Ranking of feature importance for class 2. The SHAP value represents the contribution and direction of the feature to the model prediction (positive or negative), with each point representing a sample. Red indicates a larger feature value, while blue indicates a lower feature value. More details about the cross-features (abbreviated as cross_fea in the code) can be found in TableS28. 
 The Journal of Physical Chemistry C processing and device architectures associated with HEDs based on semiconducting materials. sı Supporting Information The Supporting Information is available free of charge at https://pubs.acs.org/doi/10.1021/acs.jpcc.4c01715. Machine learning methods; introduction of descriptors, tables, and graphs for visual analysis; hyperparameter settings for models; experimental conditions and results; and images for interpretable model analysis (PDF) School of Microelectronics, University of Science and Technology of China, Hefei 230026, China; orcid.org/ 0000-0003-3089-1070; Email: qinhu20@ustc.edu.cn Yu Li -3rd Institute of Physics, University of Stuttgart, Stuttgart 70569, Germany; National Synchrotron Radiation Laboratory, University of Science and Technology of China, Hefei 230029, China; orcid.org/0000-0002-4260-1870; Email: yli1@ustc.edu.cn ■ ASSOCIATED CONTENT ■ AUTHOR INFORMATION Corresponding Authors Qin Hu - * 
			 https://doi.org/10.1021/acs.jpcc.4c01715 J. Phys. Chem. C 2024, 128, 11989-11997 Downloaded via UNIV OF CALIFORNIA SAN DIEGO on February 26, 2025 at 00:02:14 (UTC).See https://pubs.acs.org/sharingguidelines for options on how to legitimately share published articles. 
			 https://doi.org/10.1021/acs.jpcc.4c01715 J. Phys. Chem. C 2024, 128, 11989-11997 
			 The Journal of Physical Chemistry C
