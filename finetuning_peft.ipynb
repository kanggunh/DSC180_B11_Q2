{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.02s/it]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1280/1280 [00:00<00:00, 9906.61 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1280/1280 [00:00<00:00, 1344.61 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 320/320 [00:00<00:00, 9622.31 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 320/320 [00:00<00:00, 1286.82 examples/s]\n",
      "/home/ncoleban/DSC180_B11_Q2/mykernel/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training about to begin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ncoleban/DSC180_B11_Q2/mykernel/lib/python3.11/site-packages/transformers/trainer.py:3354: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1920' max='1920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1920/1920 : < :, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from accelerate import PartialState\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    GenerationConfig\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "load_dotenv()\n",
    "os.environ['WANDB_DISABLED'] = \"true\"\n",
    "\n",
    "### Getting Data Ready\n",
    "def create_prompt_formats(sample):\n",
    "    INTRO_BLURB = (\n",
    "    \"You are a scientific assistant and your task is to extract certain information from text, particularly\"\n",
    "    \"in the context of perovskite solar cells. Your task is to identify and extract details about passivating molecules and associated performance data mentioned in the text.\"\n",
    "    \"We are in a scientific environment. You MUST be critical of the units of the variables.\"\n",
    "    \"Only extract the variables that were developed in this study. You must omit the ones extracted from the bibliography\"\n",
    "    )\n",
    "    INSTRUCTION_KEY = \"\"\"### Instruct: \n",
    "    Your task is to extract relevant scientific data from the provided text about perovskite solar cells. Then join them with the data from previous chunks.\n",
    "    It is likely that a lot of this data is not present in the chunk provided. Only extract the data points that are present in the chunk.\n",
    "    Follow these guidelines:\n",
    "\n",
    "    \"Only extract the variables that were developed in this study. You must omit the ones extracted from the bibliography\"\n",
    "    Your task is to extract relevant scientific data from the provided text about perovskite solar cells.\n",
    "    Follow these guidelines:\n",
    "\n",
    "    1. **If passivating molecules are mentioned:**\n",
    "    - Do not retrieve the passivating molecule if it passivated on the electron or hole transport layers\n",
    "    - If there is more than one passivating molecule tested, only return data for the champion passivator.\n",
    "    - Include stability test data for each molecule if available. There may be multiple stability tests for a single molecule.\n",
    "\n",
    "    2. **If no passivating molecules are mentioned:**\n",
    "    - Provide a JSON object with any other relevant data explicitly mentioned in the text.\n",
    "\n",
    "    **JSON Structure:**\n",
    "    - DO NOT change the names of any of the property names. It is imperative that these are exactly as they as stated in the schema below.\n",
    "    - Ensure the output adheres to the following structure and is parseable as valid JSON:\n",
    "\n",
    "    {{\n",
    "        \"perovskite_composition\": null, // Chemical formula of the perovskite (string).\n",
    "        \"electron_transport_layer\": null, // Material used as the electron transport layer (string).\n",
    "        \"pin_nip_structure\": null, // Whether the perovskite uses a PIN or NIP structure (values: \"PIN\" or \"NIP\").\n",
    "        \"hole_transport_layer\": null, // Material used as the hole transport layer (string).\n",
    "        \"test_1\": {{ // Include only if stability tests are mentioned. Use unique keys for each test (e.g., test_1, test_2, etc.).\n",
    "            \"test_name\": null, // Must be one of: \"ISOS-D\", \"ISOS-L\", \"ISOS-T\", \"ISOS-LC\", \"ISOS-LT\".\n",
    "            \"temperature\": null, // Temperature in Celsius (numeric or string, no units or symbols like Â° or -).\n",
    "            \"time\": null, // Duration of the test in hours (string or numeric).\n",
    "            \"humidity\": null, // Humidity level (string or numeric).\n",
    "            \"retained_percentage_cont\": null, // Percentage of the PCE retained by the control perovskite after stability test (numeric) (values should be between 30-100).\n",
    "            \"retained_percentage_tret\": null, // Percentage of the PCE retained by the treated perovskite after stability test (numeric) (values should be between 30-100).\n",
    "            \"passivating_molecule\": null, // Name of the passivating molecule used in the test (must be a proper molecule name - i.e. can be parsed into SMILES format).\n",
    "            \"control_pce\": null, // Power conversion efficiency for control perovskite (numeric) (values should be between 10-30).\n",
    "            \"control_voc\": null, // Open-circuit voltage for control perovskite (numeric).\n",
    "            \"treated_pce\": null, // Power conversion efficiency for treated perovskite (numeric) (values should be between 10-30).\n",
    "            \"treated_voc\": null // Open-circuit voltage for treated perovskite (numeric).\n",
    "        }}\n",
    "    }}\n",
    "\n",
    "    **Instructions:**\n",
    "    - Be concise and accurate. Include only data explicitly present in the text.\n",
    "    - For stability tests:\n",
    "    - Infer the test type (e.g., ISOS-D, ISOS-L) based on the description if not explicitly stated.\n",
    "    - Ensure all numeric values are parseable (e.g., no symbols like Â° or -).\n",
    "    - Use unique keys for each test (e.g., `test_1`, `test_2`, etc.).\n",
    "    - If a field has no data, set it to `null`.\n",
    "    - The data may be mentioned in units different from the ones specified in the schema. In this case, convert it into the desired unit (e.g. 30 days becomes 720 hours)\n",
    "    - Make sure to only return a JSON object.\n",
    "    - Do not create any properties that are not stated in the JSON structure provided.\n",
    "    - If you cannot find a value, do not omit that property, just set it to null.\n",
    "    - Make sure not to confuse the retained_proportion_cont/retained_proportion_tret variables with the control_pce/treated_pce variables. \n",
    "    - The PCE values will almost never be above 30, while the percentage retained values will rarely be below 50%. The retained percentage will not always be there, \n",
    "    please leave these values as null if they cannot be found. DO NOT use the PCE for these values.\n",
    "\n",
    "    Text to extract from:\n",
    "\n",
    "    {chunk}\n",
    "\n",
    "    Add the newly extracted data to the one from previous chunks that is the following:\n",
    "\n",
    "    {memory}\n",
    "\n",
    "    Never leave the information from previous chunks behind.\n",
    "    \"\"\"\n",
    "    RESPONSE_KEY = \"### Output:\"\n",
    "    END_KEY = \"### End\"\n",
    "\n",
    "\n",
    "    blurb = f\"\\n{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY.format(chunk=sample['text'], memory=sample['memory'])}\"\n",
    "    response = f\"{RESPONSE_KEY}\\n{sample['output']}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "\n",
    "    parts = [part for part in [blurb, instruction, response, end] if part]\n",
    "\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "df = pd.read_csv('data/chunked_training.csv')\n",
    "dataset = Dataset.from_pandas(df).train_test_split(test_size=0.2)\n",
    "\n",
    "### Fine-tuning\n",
    "access_token = os.getenv(\"HF_TOKEN\")\n",
    "login(token=access_token)\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "# model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "device_map = \"auto\"\n",
    "device_string = PartialState().process_index\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                      device_map={'': device_string},\n",
    "                                                      quantization_config=bnb_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\", add_eos_token=True, add_bos_token=True, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "from functools import partial\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, dataset):\n",
    "\n",
    "    dataset = dataset.map(create_prompt_formats)\n",
    "\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\",\"memory\", \"output\"]\n",
    "    )\n",
    "\n",
    "    dataset = dataset.shuffle()\n",
    "\n",
    "    return dataset\n",
    "MAX_LENGTH = 2000\n",
    "\n",
    "train_dataset = preprocess_dataset(tokenizer, MAX_LENGTH, dataset['train'])\n",
    "eval_dataset = preprocess_dataset(tokenizer, MAX_LENGTH, dataset['test'])\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32, #Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        'q_proj',\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'dense'\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "original_model = prepare_model_for_kbit_training(original_model)\n",
    "original_model.gradient_checkpointing_enable()\n",
    "# original_model = torch.nn.DataParallel(original_model, device_ids=[0,1,2,3])\n",
    "\n",
    "peft_model = get_peft_model(original_model, config)\n",
    "output_dir = f'DSC180_B11_Q2/models/peft-dialogue-summary-training-{str(int(time.time()))}'\n",
    "import transformers\n",
    "\n",
    "# peft_training_args = TrainingArguments(\n",
    "#     output_dir = output_dir,\n",
    "#     warmup_steps=1,\n",
    "#     per_device_train_batch_size=1,\n",
    "#     gradient_accumulation_steps=1,\n",
    "#     max_steps=1000,\n",
    "#     learning_rate=2e-4,\n",
    "#     optim=\"paged_adamw_8bit\",\n",
    "#     lr_scheduler_type=\"cosine\",\n",
    "#     logging_steps=1,\n",
    "#     logging_dir=\"./logs\",\n",
    "#     save_strategy=\"steps\",\n",
    "#     save_steps=25,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     eval_steps=0.01,\n",
    "#     do_eval=True,\n",
    "#     gradient_checkpointing=True,\n",
    "#     report_to=\"none\",\n",
    "#     overwrite_output_dir = 'True',\n",
    "#     group_by_length=True,\n",
    "#     fp16=False,\n",
    "#     bf16=False\n",
    "# )\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    learning_rate=6e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    num_train_epochs=3,\n",
    "    fp16=False,\n",
    "    bf16=False,  # bf16 to True with an A100, False otherwise\n",
    "    logging_steps=5,  # Logging is done every step.\n",
    "    evaluation_strategy=\"steps\",\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_steps=100,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    output_dir=\"results/\",\n",
    "    logging_dir=\"DSC180_B11_Q2/logs\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=5,\n",
    "    do_eval=True,\n",
    "    save_steps=600,\n",
    "    save_total_limit=15,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "peft_model.config.use_cache = False\n",
    "\n",
    "peft_trainer = transformers.Trainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=peft_training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "print(\"training about to begin\")\n",
    "peft_trainer.train(resume_from_checkpoint=True)\n",
    "model_path = \"models/DeepSeek-R1-PSC-Extractor-8B-8bit\"\n",
    "peft_trainer.model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mykernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
