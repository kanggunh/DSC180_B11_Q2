{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ncoleban/DSC180_B11_Q2/mykernel/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    GenerationConfig\n",
    ")\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_DISABLED'] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Data Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "    INTRO_BLURB = (\n",
    "    \"You are a scientific assistant and your task is to extract certain information from text. \"\n",
    "    \"We are in a scientific environment. You MUST be critical of the units of the variables. \"\n",
    "    \"Do not leave information behind. \"\n",
    "    \"Only extract the variables that were developed in this study. You must omit the ones extracted from the bibliography\"\n",
    "    )\n",
    "    INSTRUCTION_KEY = \"\"\"### Instruct: \n",
    "    Your task is to extract relevant scientific data from the provided text about perovskite solar cells. Then join them with the data from previous chunks.\n",
    "    It is likely that a lot of this data is not present in the chunk provided. Only extract the data points that are present in the chunk.\n",
    "    Follow these guidelines:\n",
    "\n",
    "    1. **If passivating molecules are mentioned:**\n",
    "    - Create a JSON object for each passivating molecule tested.\n",
    "    - Include stability test data for each molecule if available. There may be multiple stability tests for a single molecule.\n",
    "\n",
    "    2. **If no passivating molecules are mentioned:**\n",
    "    - Provide a JSON object with any other relevant data explicitly mentioned in the text.\n",
    "\n",
    "    **JSON Structure:**\n",
    "    Ensure the output adheres to the following structure and is parseable as valid JSON:\n",
    "\n",
    "    {{\n",
    "        \"perovskite_composition\": null, // Chemical formula of the perovskite (string).\n",
    "        \"electron_transport_layer\": null, // Material used as the electron transport layer (string).\n",
    "        \"pin_nip_structure\": null, // Whether the perovskite uses a PIN or NIP structure (values: \"PIN\" or \"NIP\").\n",
    "        \"hole_transport_layer\": null, // Material used as the hole transport layer (string).\n",
    "        \"test_1\": {{ // Include only if stability tests are mentioned. Use unique keys for each test (e.g., test_1, test_2, etc.).\n",
    "            \"test_name\": null, // Must be one of: \"ISOS-D\", \"ISOS-L\", \"ISOS-T\", \"ISOS-LC\", \"ISOS-LT\".\n",
    "            \"temperature\": null, // Temperature in Celsius (numeric or string, no units or symbols like Â° or -).\n",
    "            \"time\": null, // Duration of the test (string or numeric).\n",
    "            \"humidity\": null, // Humidity level (string or numeric).\n",
    "            \"control_efficiency\": null, // Efficiency of the control sample (numeric).\n",
    "            \"treatment_efficiency\": null, // Efficiency of the treated sample (numeric).\n",
    "            \"passivating_molecule\": null, // Name of the passivating molecule used in the test.\n",
    "            \"control_pce\": null, // Power conversion efficiency for control perovskite (numeric).\n",
    "            \"control_voc\": null, // Open-circuit voltage for control perovskite (numeric).\n",
    "            \"treated_pce\": null, // Power conversion efficiency for treated perovskite (numeric).\n",
    "            \"treated_voc\": null // Open-circuit voltage for treated perovskite (numeric).\n",
    "        }}\n",
    "    }}\n",
    "\n",
    "    **Instructions:**\n",
    "    - Be concise and accurate. Include only data explicitly present in the text.\n",
    "    - For stability tests:\n",
    "    - Infer the test type (e.g., ISOS-D, ISOS-L) based on the description if not explicitly stated.\n",
    "    - Ensure all numeric values are parseable (e.g., no symbols like Â° or -).\n",
    "    - Use unique keys for each test (e.g., `test_1`, `test_2`, etc.).\n",
    "    - If a field has no data, set it to `null`.\n",
    "\n",
    "    Text to extract from:\n",
    "\n",
    "    {chunk}\n",
    "\n",
    "    Add the newly extracted data to the one from previous chunks that is the following:\n",
    "\n",
    "    {memory}\n",
    "\n",
    "    Never leave the information from previous chunks behind.\n",
    "    \"\"\"\n",
    "    RESPONSE_KEY = \"### Output:\"\n",
    "    END_KEY = \"### End\"\n",
    "\n",
    "\n",
    "    blurb = f\"\\n{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY.format(chunk=sample['text'], memory=sample['memory'])}\"\n",
    "    response = f\"{RESPONSE_KEY}\\n{sample['output']}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "\n",
    "    parts = [part for part in [blurb, instruction, response, end] if part]\n",
    "\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/chunked_training.csv')\n",
    "dataset = Dataset.from_pandas(df).train_test_split(test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "access_token = os.getenv(\"HF_TOKEN\")\n",
    "login(token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.49s/it]\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "device_map = \"auto\"\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                      device_map=device_map,\n",
    "                                                      quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\", add_eos_token=True, add_bos_token=True, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, dataset):\n",
    "\n",
    "    dataset = dataset.map(create_prompt_formats)\n",
    "\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\",\"memory\", \"output\"]\n",
    "    )\n",
    "\n",
    "    dataset = dataset.shuffle()\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1275/1275 [00:00<00:00, 10831.30 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1275/1275 [00:00<00:00, 1616.04 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 319/319 [00:00<00:00, 10970.58 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 319/319 [00:00<00:00, 1671.79 examples/s]\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 2000\n",
    "\n",
    "train_dataset = preprocess_dataset(tokenizer, MAX_LENGTH, dataset['train'])\n",
    "eval_dataset = preprocess_dataset(tokenizer, MAX_LENGTH, dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32, #Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        'q_proj',\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'dense'\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "original_model = prepare_model_for_kbit_training(original_model)\n",
    "original_model.gradient_checkpointing_enable()\n",
    "# original_model = torch.nn.DataParallel(original_model, device_ids=[0,1,2,3])\n",
    "\n",
    "peft_model = get_peft_model(original_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ncoleban/DSC180_B11_Q2/mykernel/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training about to begin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ncoleban/DSC180_B11_Q2/mykernel/lib/python3.11/site-packages/transformers/trainer.py:3354: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \n",
      "\tsave_steps: 200 (from args) != 10 (from trainer_state.json)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1595' max='1595' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1595/1595 : < :, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'save_pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/DeepSeek-R1-PSC-Extractor-8B\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m peft_trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(model_path)\n\u001b[0;32m---> 40\u001b[0m \u001b[43mpeft_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m(model_path)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'save_pretrained'"
     ]
    }
   ],
   "source": [
    "peft_training_args = TrainingArguments(\n",
    "    learning_rate=6e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    num_train_epochs=5,\n",
    "    fp16=False,\n",
    "    bf16=False,  # bf16 to True with an A100, False otherwise\n",
    "    logging_steps=5,  # Logging is done every step.\n",
    "    evaluation_strategy=\"steps\",\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_steps=100,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    output_dir=\"results/\",\n",
    "    logging_dir=\"logs\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=5,\n",
    "    do_eval=True,\n",
    "    save_steps=200,\n",
    "    save_total_limit=10,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "peft_model.config.use_cache = False\n",
    "\n",
    "peft_trainer = transformers.Trainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=peft_training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "print(\"training about to begin\")\n",
    "peft_trainer.train(resume_from_checkpoint=True)\n",
    "model_path = \"models/DeepSeek-R1-PSC-Extractor-8B\"\n",
    "peft_trainer.model.save_pretrained(model_path)\n",
    "peft_trainer.tokenizer.save_pretrained(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mykernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
