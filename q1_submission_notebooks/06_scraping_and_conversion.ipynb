{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import requests\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "from io import BytesIO\n",
    "import pymupdf\n",
    "import os\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import tempfile\n",
    "import time\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import grobid_tei_xml\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_papers = pd.read_csv(\"../data/good_paper_links.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping with Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = tempfile.mkdtemp()\n",
    "chrome_options = uc.ChromeOptions()\n",
    "chrome_options.add_experimental_option(\"prefs\", {\n",
    "    \"download.default_directory\": download_dir,  # Set download location\n",
    "    \"download.prompt_for_download\": False,       # Disable download prompts\n",
    "    \"plugins.always_open_pdf_externally\": True   # Download PDFs instead of opening them\n",
    "})\n",
    "driver = uc.Chrome(options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\nicco\\\\AppData\\\\Local\\\\Temp\\\\tmpnpfzn9o3'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_url(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    # Reconstruct URL without query parameters and fragment\n",
    "    return urlunparse((parsed_url.scheme, parsed_url.netloc, parsed_url.path, '', '', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_pdf_if_button(driver):\n",
    "    try:\n",
    "        driver.find_element(By.XPATH, \"//embed[contains(@type, 'application/pdf')]\")\n",
    "        return True\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        for iframe in driver.find_elements(By.TAG_NAME, \"iframe\"):\n",
    "            try:\n",
    "                frame_type = iframe.get_attribute(\"type\")\n",
    "                if frame_type == \"application/pdf\":\n",
    "                    driver.get(iframe.get_attribute(\"src\"))\n",
    "                    return True\n",
    "            except:\n",
    "                print(f\"Failed to get link {iframe}\")\n",
    "                return False\n",
    "    except:\n",
    "        print(\"No open button found for current PDF\")\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf_urls(url, paper_index):\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    pdfs_unique = set()\n",
    "    pdf_links = []\n",
    "    pdf_pattern = re.compile(r'(?<!e)\\.pdf$|/pdf/|/articlepdf/|/article-pdf/', re.IGNORECASE)\n",
    "    for link in driver.find_elements(By.TAG_NAME, \"a\"):\n",
    "        try:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"scholar.google\" not in href and pdf_pattern.search(href): #selenium cannot download epdfs\n",
    "                base_url = get_base_url(href)\n",
    "                if base_url not in pdfs_unique:\n",
    "                    pdfs_unique.add(base_url)\n",
    "                    pdf_links.append(href)\n",
    "        except:\n",
    "            print(f\"Failed to get link {link}\")\n",
    "    merged_pdf = pymupdf.open()\n",
    "    i = 0\n",
    "    if len(pdf_links) == 0:\n",
    "        print(f\"No PDF links found for paper {url}\")\n",
    "        return\n",
    "    downloadable_links_count = 0\n",
    "    for pdf_link in pdf_links:\n",
    "        # Ensure each link is a full URL\n",
    "        pdf_url = pdf_link if pdf_link.startswith('http') else get_base_url(url) + pdf_link\n",
    "        if \"pdf\" not in pdf_url: #skips non-pdfs after base url is used\n",
    "            continue\n",
    "        try:\n",
    "            num_of_files_prev = len([f for f in os.listdir(download_dir)])\n",
    "            curr_url = driver.current_url\n",
    "            driver.get(pdf_url)\n",
    "            if curr_url != driver.current_url: # redirected to another page\n",
    "                open_pdf_if_button(driver)\n",
    "            time.sleep(1)\n",
    "            num_of_files_now = len([f for f in os.listdir(download_dir)])\n",
    "            downloadable_links_count += num_of_files_now > num_of_files_prev\n",
    "        except:\n",
    "            print(f\"Skipping invalid PDF at {pdf_url}\")\n",
    "            continue\n",
    "    downloaded_pdfs = [f for f in os.listdir(download_dir) if f.endswith('.pdf')]\n",
    "    print(downloaded_pdfs)\n",
    "    while len(downloaded_pdfs) < downloadable_links_count:\n",
    "        time.sleep(1)\n",
    "        downloaded_pdfs = [f for f in os.listdir(download_dir) if f.endswith('.pdf')]\n",
    "        \n",
    "    pdf_files = [os.path.join(download_dir, f) for f in os.listdir(download_dir) if f.endswith('.pdf')]\n",
    "    output_path = f'../../data/pdfs/{paper_index}.pdf'\n",
    "    for pdf in pdf_files:\n",
    "        print(pdf)\n",
    "        merged_pdf.insert_pdf(pymupdf.open(pdf))\n",
    "    merged_pdf.save(output_path)\n",
    "    merged_pdf.close()\n",
    "    \n",
    "    for pdf in pdf_files:\n",
    "        os.remove(pdf)\n",
    "    print(f\"Merged PDF saved as {output_path}\")\n",
    "    return pdf_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in good_papers.iterrows():\n",
    "    download_pdf_urls(row['Link'], index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: use base url for checking duplicates but the actual url for downloading (reference i = 12)\n",
    "TODO: not getting links for i = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert PDFs to XML using GROBID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to convert 38.pdf. Status code: 500\n",
      "[BAD_INPUT_DATA] PDF to XML conversion failed with error code: 139\n"
     ]
    }
   ],
   "source": [
    "grobid_url = \"http://localhost:8070/api/processFulltextDocument\"\n",
    "xml_names = os.listdir(\"../../data/xmls\")\n",
    "\n",
    "for pdf_file in os.listdir(\"../../data/pdfs\"):\n",
    "    #only looks at pdf files\n",
    "    if pdf_file.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(\"../../data/pdfs\", pdf_file)\n",
    "        #doe not convert already converted files\n",
    "        if pdf_path.replace('.pdf', '.xml') in xml_names:\n",
    "            continue\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            #GROBID must be running on port 8070 for this to work\n",
    "            response = requests.post(\n",
    "                grobid_url,\n",
    "                files={'input': file},\n",
    "                headers={'Accept': 'application/xml'}\n",
    "            )\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                xml_file_path = os.path.join('../../data/xmls', pdf_file.replace('.pdf', '.xml'))\n",
    "                with open(xml_file_path, 'w', encoding='utf-8') as xml_file:\n",
    "                    xml_file.write(response.text)\n",
    "            else:\n",
    "                print(f\"Failed to convert {pdf_file}. Status code: {response.status_code}\")\n",
    "                print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_to_text(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    print(file_path)\n",
    "    print(root.find(\".//title\"))\n",
    "    title = \"\"\n",
    "    sections = []\n",
    "    figures = []\n",
    "    \n",
    "    namespace = { 'd': root.tag.split('}')[0].strip('{') if '}' in root.tag else '' }\n",
    "    def ns_tag(tag):\n",
    "        return f\"{{{namespace}}}tag\" if namespace else tag\n",
    "\n",
    "    print(namespace)\n",
    "    namespace = { 'd': \"http://www.tei-c.org/ns/1.0\" }\n",
    "    title_element = root.find('d:title', namespace)\n",
    "    if title_element is not None:\n",
    "        title = title_element.text\n",
    "    \n",
    "    for div in root.findall(\".//div\"):\n",
    "        section_text = \"/n\".join(div.itertext())\n",
    "        sections.append(section_text)\n",
    "    \n",
    "    for figure in root.findall(\".//figure\"):\n",
    "        fig_head = figure.find(\".//head\")\n",
    "        fig_description = figure.find(\".//figDesc\")\n",
    "        fig_info = (fig_head.text if fig_head else \"Fig:\") + \" \" \n",
    "        + (fig_description.text if fig_description else \"unkown description\")\n",
    "        figures.append(fig_info)\n",
    "    return title + \"/n\" + \"/n\".join(sections) + \"/n\" + \"/n\".join(figures)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_grobid_xml(file_path):\n",
    "    with open(file_path, \"r\") as xml_file:\n",
    "        doc = grobid_tei_xml.parse_document_xml(xml_file.read())\n",
    "        title = doc.header.title or \"\"\n",
    "        abstract = doc.abstract or \"\"\n",
    "        body = doc.body or \"\"\n",
    "        index = file_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        return f\"Paper #: {index}\\n{title}\\n{abstract}\\n{body}\" #title, abstract, body\n",
    "    # print(json.dumps(doc.to_dict(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.xml\n",
      "63.xml\n",
      "77.xml\n",
      "76.xml\n",
      "62.xml\n",
      "89.xml\n",
      "149.xml\n",
      "48.xml\n",
      "74.xml\n",
      "60.xml\n",
      "61.xml\n",
      "75.xml\n",
      "49.xml\n",
      "148.xml\n",
      "71.xml\n",
      "65.xml\n",
      "59.xml\n",
      "58.xml\n",
      "64.xml\n",
      "70.xml\n",
      "99.xml\n",
      "66.xml\n",
      "72.xml\n",
      "8.xml\n",
      "9.xml\n",
      "73.xml\n",
      "67.xml\n",
      "98.xml\n",
      "101.xml\n",
      "115.xml\n",
      "129.xml\n",
      "28.xml\n",
      "14.xml\n",
      "15.xml\n",
      "29.xml\n",
      "128.xml\n",
      "114.xml\n",
      "100.xml\n",
      "116.xml\n",
      "102.xml\n",
      "17.xml\n",
      "16.xml\n",
      "103.xml\n",
      "117.xml\n",
      "113.xml\n",
      "107.xml\n",
      "12.xml\n",
      "13.xml\n",
      "106.xml\n",
      "112.xml\n",
      "138.xml\n",
      "104.xml\n",
      "110.xml\n",
      "11.xml\n",
      "39.xml\n",
      "10.xml\n",
      "111.xml\n",
      "105.xml\n",
      "139.xml\n",
      "120.xml\n",
      "134.xml\n",
      "108.xml\n",
      "21.xml\n",
      "35.xml\n",
      "34.xml\n",
      "20.xml\n",
      "109.xml\n",
      "135.xml\n",
      "121.xml\n",
      "137.xml\n",
      "123.xml\n",
      "36.xml\n",
      "22.xml\n",
      "23.xml\n",
      "37.xml\n",
      "122.xml\n",
      "136.xml\n",
      "132.xml\n",
      "126.xml\n",
      "33.xml\n",
      "27.xml\n",
      "26.xml\n",
      "32.xml\n",
      "127.xml\n",
      "133.xml\n",
      "119.xml\n",
      "125.xml\n",
      "131.xml\n",
      "24.xml\n",
      "30.xml\n",
      "18.xml\n",
      "19.xml\n",
      "31.xml\n",
      "25.xml\n",
      "130.xml\n",
      "124.xml\n",
      "118.xml\n",
      "143.xml\n",
      "81.xml\n",
      "95.xml\n",
      "42.xml\n",
      "56.xml\n",
      "4.xml\n",
      "57.xml\n",
      "5.xml\n",
      "43.xml\n",
      "94.xml\n",
      "80.xml\n",
      "142.xml\n",
      "140.xml\n",
      "96.xml\n",
      "82.xml\n",
      "69.xml\n",
      "7.xml\n",
      "55.xml\n",
      "41.xml\n",
      "40.xml\n",
      "6.xml\n",
      "54.xml\n",
      "68.xml\n",
      "83.xml\n",
      "97.xml\n",
      "141.xml\n",
      "93.xml\n",
      "87.xml\n",
      "145.xml\n",
      "50.xml\n",
      "2.xml\n",
      "44.xml\n",
      "78.xml\n",
      "79.xml\n",
      "45.xml\n",
      "51.xml\n",
      "3.xml\n",
      "144.xml\n",
      "86.xml\n",
      "92.xml\n",
      "84.xml\n",
      "90.xml\n",
      "146.xml\n",
      "47.xml\n",
      "1.xml\n",
      "53.xml\n",
      "0.xml\n",
      "52.xml\n",
      "46.xml\n",
      "147.xml\n",
      "91.xml\n",
      "85.xml\n"
     ]
    }
   ],
   "source": [
    "xml_dir = \"../../data/xmls\"\n",
    "txt_dir = \"../../data/txts\"\n",
    "for filename in os.listdir(\"../../data/xmls\"):\n",
    "    if filename.endswith(\".xml\"):\n",
    "        print(filename)\n",
    "        txt_content = parse_grobid_xml(os.path.join(xml_dir, filename))\n",
    "        txt_file = os.path.join(txt_dir, f\"{os.path.splitext(filename)[0]}.txt\")\n",
    "        with open(txt_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(txt_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Link    https://doi.org/10.1063%2F1.1840606\n",
       "Name: 34, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_papers.iloc[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://doi.org/10.1063%2F1.1840606\n",
      "['4590_1_online.pdf']\n",
      "C:\\Users\\nicco\\AppData\\Local\\Temp\\tmpnpfzn9o3\\4590_1_online.pdf\n",
      "C:\\Users\\nicco\\AppData\\Local\\Temp\\tmpnpfzn9o3\\ContentPlatform_UserGuide_FINAL.pdf\n",
      "Merged PDF saved as ../../data/pdfs/34.pdf\n"
     ]
    }
   ],
   "source": [
    "for index, row in good_papers.iterrows():\n",
    "    if index == 34:\n",
    "        print(row['Link'])\n",
    "        download_pdf_urls(row['Link'], index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
